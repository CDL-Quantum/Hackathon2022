{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9190faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pennylane import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pennylane as qml\n",
    "#from pennylane_qiskit import IBMQDevice\n",
    "#from pennylane_qiskit import BasicAerDevice\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c45f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, data, target, sample_size = 0, test_split = 0.3, seed = 10):\n",
    "        self.data = data\n",
    "        self.preprocess_done = None\n",
    "        \n",
    "        if sample_size == 0:\n",
    "            self.data_sample = data\n",
    "        else:\n",
    "            self.data_sample = data.sample(sample_size)\n",
    "            \n",
    "        self.train_set, self.test_set = train_test_split(self.data_sample, test_size=test_split, random_state=seed)\n",
    "        \n",
    "        self.y_train = self.train_set[[target]]\n",
    "        self.y_test = self.test_set[[target]]\n",
    "        \n",
    "        self.x_train = self.train_set.drop(target, axis=1)\n",
    "        self.x_test = self.test_set.drop(target, axis=1)\n",
    "        \n",
    "    def view_info(self):\n",
    "        print(self.data_sample.info())\n",
    "        if self.preprocess_done == None:\n",
    "            print(\"No preprocessing done yet.\")\n",
    "        else:\n",
    "            print(\"Preprocessing done via: \", self.preprocess_done)\n",
    "        return self.data_sample.describe()\n",
    "    \n",
    "    def perform_LDA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"LDA\"\n",
    "        \n",
    "        length = len(self.x_train.columns)\n",
    "        split_feature = int(length/n_dim)\n",
    "        features_train = []\n",
    "        features_test = []\n",
    "        \n",
    "        for i in range(n_dim):\n",
    "            new_set_train = self.x_train.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_train.append(new_set_train)\n",
    "            \n",
    "            new_set_test = self.x_test.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_test.append(new_set_test)\n",
    "        \n",
    "        lda = LDA(n_components= (n_dim - 1))\n",
    "        features_lda_train = []\n",
    "        features_lda_test = []\n",
    "        \n",
    "        for i in range(n_dim):\n",
    "            features_lda_train_new = lda.fit_transform(features_train[i], self.y_train)\n",
    "            features_lda_train.append(pd.DataFrame(features_lda_train_new))\n",
    "            \n",
    "            features_lda_test_new = lda.fit_transform(features_test[i], self.y_test)\n",
    "            features_lda_test.append(pd.DataFrame(features_lda_test_new))\n",
    "            \n",
    "        Y = np.array(y_train.values[:,0] * 2 - np.ones(len(y_train.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        X = np.array(data, requires_grad=False)\n",
    "        \n",
    "        \n",
    "            \n",
    "    def perform_PCA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"PCA\"\n",
    "        \n",
    "        y_train.value_counts(normalize=True)*100\n",
    "        y_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        pca.fit(x_train)\n",
    "        x_train_pca = pca.transform(x_train)\n",
    "        pca.fit(x_test)\n",
    "        x_test = pca.transform(x_test)\n",
    "        \n",
    "        data = normalize(x_train_pca)\n",
    "        x_test = normalize(x_test)\n",
    "        \n",
    "        Y = np.array(y_train.values[:,0] * 2 - np.ones(len(y_train.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        X = np.array(data, requires_grad=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4393f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fraud_detection_bank_dataset.csv', sep=',')\n",
    "df = df.astype(float)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f48c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(data = df,target = \"targets\", sample_size = 2000, test_split = 0.3, seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dddfb6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 9695 to 4714\n",
      "Columns: 113 entries, col_0 to targets\n",
      "dtypes: float64(113)\n",
      "memory usage: 1.7 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>col_103</th>\n",
       "      <th>col_104</th>\n",
       "      <th>col_105</th>\n",
       "      <th>col_106</th>\n",
       "      <th>col_107</th>\n",
       "      <th>col_108</th>\n",
       "      <th>col_109</th>\n",
       "      <th>col_110</th>\n",
       "      <th>col_111</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.092500</td>\n",
       "      <td>282.785500</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>1.112500</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>3.092500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>44.923000</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.300566</td>\n",
       "      <td>612.116292</td>\n",
       "      <td>1.874709</td>\n",
       "      <td>5.852797</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>6.910334</td>\n",
       "      <td>2.972607</td>\n",
       "      <td>14.300566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>0.487872</td>\n",
       "      <td>0.04995</td>\n",
       "      <td>0.461782</td>\n",
       "      <td>0.398212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215922</td>\n",
       "      <td>0.354241</td>\n",
       "      <td>60.920298</td>\n",
       "      <td>0.444329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>285.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>420.000000</td>\n",
       "      <td>12712.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_0         col_1        col_2        col_3        col_4  \\\n",
       "count  2000.000000   2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      3.092500    282.785500     0.315000     2.130000     0.073500   \n",
       "std      14.300566    612.116292     1.874709     5.852797     0.857009   \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     39.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000    100.000000     0.000000     1.000000     0.000000   \n",
       "75%       2.000000    285.500000     0.000000     2.000000     0.000000   \n",
       "max     420.000000  12712.000000    48.000000   105.000000    35.000000   \n",
       "\n",
       "             col_5        col_6        col_7   col_8   col_9  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.0  2000.0  ...   \n",
       "mean      1.112500     2.355000     3.092500     0.0     0.0  ...   \n",
       "std       6.910334     2.972607    14.300566     0.0     0.0  ...   \n",
       "min       0.000000    -1.000000     0.000000     0.0     0.0  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.0     0.0  ...   \n",
       "50%       0.000000     2.000000     0.000000     0.0     0.0  ...   \n",
       "75%       1.000000     6.000000     2.000000     0.0     0.0  ...   \n",
       "max     230.000000     8.000000   420.000000     0.0     0.0  ...   \n",
       "\n",
       "           col_103      col_104     col_105      col_106      col_107  \\\n",
       "count  2000.000000  2000.000000  2000.00000  2000.000000  2000.000000   \n",
       "mean      0.003500     0.390000     0.00250     0.308000     0.197500   \n",
       "std       0.059072     0.487872     0.04995     0.461782     0.398212   \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "75%       0.000000     1.000000     0.00000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.00000     1.000000     1.000000   \n",
       "\n",
       "       col_108      col_109      col_110      col_111      targets  \n",
       "count   2000.0  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean       0.0     0.049000     0.024000    44.923000     0.270500  \n",
       "std        0.0     0.215922     0.354241    60.920298     0.444329  \n",
       "min        0.0     0.000000     0.000000     0.000000     0.000000  \n",
       "25%        0.0     0.000000     0.000000     5.000000     0.000000  \n",
       "50%        0.0     0.000000     0.000000    21.000000     0.000000  \n",
       "75%        0.0     0.000000     0.000000    62.000000     1.000000  \n",
       "max        0.0     1.000000     9.000000   747.000000     1.000000  \n",
       "\n",
       "[8 rows x 113 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb47bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBC:\n",
    "    def __init__(self, Data, n_dim, n_layers, \n",
    "                 optimizer = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08),\n",
    "                 loss_function = None, batch_size = 10, backend = \"default.qubit\", shots = 0):\n",
    "        \n",
    "        if loss_function == None:\n",
    "            self.loss_function = square_loss\n",
    "        else: \n",
    "            self.loss_function = loss_function\n",
    "        self.opt = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        dev = qml.device(backend, wires = self.n_dim, shots=shots)\n",
    "        #dev = qml.device('default.qubit.tf', wires = num_qubits, shots=1024)\n",
    "        #dev = qml.device('qiskit.ibmq', wires = num_qubits, backend='ibmq_manila', ibmqx_token=\"6cc75c58fc80fea56cb8dd391f8fbcfdb676a3dc7005493728bc9da7ea753e31a2110a01e3a0cc83f1a98f5ca79e32956fc66c11b5eea4cae163b3fa996be356\", shots=256)\n",
    "        #dev = qml.device('qiskit.basicaer', wires = num_qubits, shots = 256)\n",
    "\n",
    "        @qml.qnode(dev)\n",
    "        def circuit(parameters, data):\n",
    "            for i in range(num_qubits):\n",
    "                qml.Hadamard(wires = i)\n",
    "\n",
    "            AngleEmbedding(features = data, wires = range(num_qubits), rotation = 'Y')\n",
    "\n",
    "            qml.StronglyEntanglingLayers(weights = parameters, wires = range(num_qubits))\n",
    "\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.weights = 0.01 * np.random.randn(self.n_layers, self.n_dim, 3, requires_grad=True)\n",
    "        self.bias = np.array(0.0, requires_grad=True)\n",
    "        \n",
    "    def variational_classifier(weights, bias, x):\n",
    "        return circuit(weights, x) + bias\n",
    "    \n",
    "    def square_loss(labels, predictions):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            loss = loss + (l - p) ** 2\n",
    "\n",
    "        loss = loss / len(labels)\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < 1e-5:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def cost(weights, bias, X, Y):\n",
    "        predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "        return self.loss_function(Y, predictions)\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        wbest = 0\n",
    "        bbest = 0\n",
    "        abest = 0\n",
    "\n",
    "        for it in range(n_epochs):\n",
    "\n",
    "            # weights update by one optimizer step\n",
    "\n",
    "            batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "            X_batch = X[batch_index]\n",
    "            Y_batch = Y[batch_index]\n",
    "            weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "            if accuracy(Y, predictions) > abest:\n",
    "                wbest = weights\n",
    "                bbest = bias\n",
    "                abest = accuracy(Y, predictions)\n",
    "                print('New best')\n",
    "\n",
    "            acc = accuracy(Y, predictions)\n",
    "\n",
    "            print(\n",
    "                \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "                    it + 1, cost(weights, bias, X, Y), acc\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41542144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pennylane import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pennylane as qml\n",
    "#from pennylane_qiskit import IBMQDevice\n",
    "#from pennylane_qiskit import BasicAerDevice\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0240b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, data, target, sample_size = 0, test_split = 0.3, seed = 10):\n",
    "        self.data = data\n",
    "        self.preprocess_done = None\n",
    "        \n",
    "        if sample_size == 0:\n",
    "            self.data_sample = data\n",
    "        else:\n",
    "            self.data_sample = data.sample(sample_size)\n",
    "            \n",
    "        self.train_set, self.test_set = train_test_split(self.data_sample, test_size=test_split, random_state=seed)\n",
    "        \n",
    "        self.y_train = self.train_set[[target]]\n",
    "        self.y_test = self.test_set[[target]]\n",
    "        \n",
    "        self.x_train = self.train_set.drop(target, axis=1)\n",
    "        self.x_test = self.test_set.drop(target, axis=1)\n",
    "        \n",
    "    def view_info(self):\n",
    "        print(self.data_sample.info())\n",
    "        if self.preprocess_done == None:\n",
    "            print(\"No preprocessing done yet.\")\n",
    "        else:\n",
    "            print(\"Preprocessing done via: \", self.preprocess_done)\n",
    "        return self.data_sample.describe()\n",
    "    \n",
    "    def view_preprocessed(self):\n",
    "        if self.preprocess_done == None:\n",
    "            print(\"Please do some preprocessing first.\")\n",
    "        else:\n",
    "            print(\"Training Set and Labels: \")\n",
    "            print(self.train_X_preprocessed)\n",
    "            print(self.train_Y_preprocessed)\n",
    "\n",
    "            print(\"Test Set and Labels: \")\n",
    "            print(self.test_X_preprocessed)\n",
    "            print(self.test_Y_preprocessed)\n",
    "    \n",
    "    def perform_LDA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"LDA\"\n",
    "        print(\"Performing LDA...\")\n",
    "        \n",
    "        length = len(self.x_train.columns)\n",
    "        split_feature = int(length/n_dim)\n",
    "        features_train = []\n",
    "        features_test = []\n",
    "        \n",
    "        # Split Features (for Yaqi to change)\n",
    "        for i in range(n_dim):\n",
    "            new_set_train = self.x_train.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_train.append(new_set_train)\n",
    "            \n",
    "            new_set_test = self.x_test.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_test.append(new_set_test)\n",
    "        \n",
    "        # Run the LDA\n",
    "        lda = LDA(n_components= (n_dim - 1))\n",
    "        features_lda_train = []\n",
    "        features_lda_test = []\n",
    "        \n",
    "        for i in range(n_dim):\n",
    "            features_lda_train_new = lda.fit_transform(features_train[i], self.y_train)\n",
    "            features_lda_train.append(pd.DataFrame(features_lda_train_new))\n",
    "            \n",
    "            features_lda_test_new = lda.fit_transform(features_test[i], self.y_test)\n",
    "            features_lda_test.append(pd.DataFrame(features_lda_test_new))\n",
    "        \n",
    "        x_train_data = features_lda_train[0]\n",
    "        x_test_data = features_lda_test[0]\n",
    "        \n",
    "        # Join the results together\n",
    "        for i in range(1, n_dim):\n",
    "            l_suffix = \"_\" + str(i)\n",
    "            r_suffix = \"_\" + str(i+1)\n",
    "            x_train_data = x_train_data.join(features_lda_train[i], lsuffix=l_suffix, rsuffix=r_suffix)\n",
    "            x_test_data = x_test_data.join(features_lda_test[i], lsuffix=l_suffix, rsuffix=r_suffix)\n",
    "        \n",
    "        # Normalize\n",
    "        std_scale_train = StandardScaler().fit(x_train_data)\n",
    "        x_train_data = std_scale_train.transform(x_train_data)\n",
    "        \n",
    "        std_scale_test = StandardScaler().fit(x_test_data)\n",
    "        x_test_data = std_scale_test.transform(x_test_data)\n",
    "            \n",
    "        # shift label from {0, 1} to {-1, 1}\n",
    "        self.train_X_preprocessed = np.array(x_train_data, requires_grad=False)\n",
    "        self.train_Y_preprocessed = np.array(self.y_train.values[:,0] * 2 - np.ones(len(self.y_train.values[:,0])), requires_grad = False)\n",
    "        \n",
    "        self.test_X_preprocessed = np.array(x_test_data, requires_grad=False)\n",
    "        self.test_Y_preprocessed = np.array(self.y_test.values[:,0] * 2 - np.ones(len(self.y_test.values[:,0])), requires_grad = False)\n",
    "        \n",
    "    def perform_PCA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"PCA\"\n",
    "        print(\"Performing PCA...\")\n",
    "        \n",
    "        self.y_train.value_counts(normalize=True)*100\n",
    "        self.y_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        pca.fit(self.x_train)\n",
    "        x_train_pca = pca.transform(self.x_train)\n",
    "        pca.fit(self.x_test)\n",
    "        x_test = pca.transform(self.x_test)\n",
    "        \n",
    "        train_X_preprocessed = normalize(self.x_train_pca)\n",
    "        test_X_preprocessed = normalize(self.x_test)\n",
    "        \n",
    "        self.train_Y_preprocessed = np.array(self.y_train.values[:,0] * 2 - np.ones(len(self.y_train.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        self.train_X_preprocessed = np.array(train_X_preprocessed, requires_grad=False)\n",
    "        \n",
    "        self.test_Y_preprocessed = np.array(self.y_test.values[:,0] * 2 - np.ones(len(self.y_test.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        self.test_X_preprocessed = np.array(test_X_preprocessed, requires_grad=False)\n",
    "        \n",
    "    def perform_normalize(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"Normalize\"\n",
    "        print(\"Performing Normalize...\")\n",
    "        \n",
    "        self.y_train.value_counts(normalize=True)*100\n",
    "        self.y_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        self.x_train.value_counts(normalize=True)*100\n",
    "        self.x_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6162ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fraud_detection_bank_dataset.csv', sep=',')\n",
    "df = df.astype(float)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38e42694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PrepareData(data = df,target = \"targets\", sample_size = 2000, test_split = 0.3, seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e769ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 9997 to 10438\n",
      "Columns: 113 entries, col_0 to targets\n",
      "dtypes: float64(113)\n",
      "memory usage: 1.7 MB\n",
      "None\n",
      "No preprocessing done yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>col_103</th>\n",
       "      <th>col_104</th>\n",
       "      <th>col_105</th>\n",
       "      <th>col_106</th>\n",
       "      <th>col_107</th>\n",
       "      <th>col_108</th>\n",
       "      <th>col_109</th>\n",
       "      <th>col_110</th>\n",
       "      <th>col_111</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.089000</td>\n",
       "      <td>284.03150</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>2.346000</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>2.543000</td>\n",
       "      <td>3.089000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04550</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>42.191500</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.594584</td>\n",
       "      <td>525.37318</td>\n",
       "      <td>1.345882</td>\n",
       "      <td>8.883453</td>\n",
       "      <td>0.406942</td>\n",
       "      <td>3.193977</td>\n",
       "      <td>3.036406</td>\n",
       "      <td>10.594584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073976</td>\n",
       "      <td>0.475812</td>\n",
       "      <td>0.04995</td>\n",
       "      <td>0.460945</td>\n",
       "      <td>0.385528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20845</td>\n",
       "      <td>0.219326</td>\n",
       "      <td>60.283662</td>\n",
       "      <td>0.435123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>307.25000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>182.000000</td>\n",
       "      <td>7457.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_0       col_1        col_2        col_3        col_4  \\\n",
       "count  2000.000000  2000.00000  2000.000000  2000.000000  2000.000000   \n",
       "mean      3.089000   284.03150     0.277500     2.346000     0.059000   \n",
       "std      10.594584   525.37318     1.345882     8.883453     0.406942   \n",
       "min       0.000000     0.00000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000    38.00000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000    98.50000     0.000000     1.000000     0.000000   \n",
       "75%       2.000000   307.25000     0.000000     2.000000     0.000000   \n",
       "max     182.000000  7457.00000    25.000000   179.000000    11.000000   \n",
       "\n",
       "             col_5        col_6        col_7   col_8   col_9  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.0  2000.0  ...   \n",
       "mean      0.885500     2.543000     3.089000     0.0     0.0  ...   \n",
       "std       3.193977     3.036406    10.594584     0.0     0.0  ...   \n",
       "min       0.000000    -1.000000     0.000000     0.0     0.0  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.0     0.0  ...   \n",
       "50%       0.000000     2.000000     0.000000     0.0     0.0  ...   \n",
       "75%       1.000000     6.000000     2.000000     0.0     0.0  ...   \n",
       "max      79.000000     8.000000   182.000000     0.0     0.0  ...   \n",
       "\n",
       "           col_103      col_104     col_105      col_106      col_107  \\\n",
       "count  2000.000000  2000.000000  2000.00000  2000.000000  2000.000000   \n",
       "mean      0.005500     0.346000     0.00250     0.306000     0.181500   \n",
       "std       0.073976     0.475812     0.04995     0.460945     0.385528   \n",
       "min       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.00000     0.000000     0.000000   \n",
       "75%       0.000000     1.000000     0.00000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.00000     1.000000     1.000000   \n",
       "\n",
       "       col_108     col_109      col_110      col_111      targets  \n",
       "count   2000.0  2000.00000  2000.000000  2000.000000  2000.000000  \n",
       "mean       0.0     0.04550     0.020500    42.191500     0.253500  \n",
       "std        0.0     0.20845     0.219326    60.283662     0.435123  \n",
       "min        0.0     0.00000     0.000000     0.000000     0.000000  \n",
       "25%        0.0     0.00000     0.000000     5.000000     0.000000  \n",
       "50%        0.0     0.00000     0.000000    14.000000     0.000000  \n",
       "75%        0.0     0.00000     0.000000    62.000000     1.000000  \n",
       "max        0.0     1.00000     5.000000   567.000000     1.000000  \n",
       "\n",
       "[8 rows x 113 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97307aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "data.perform_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35b2cb3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/1375028561.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_preprocessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/3634425272.py\u001b[0m in \u001b[0;36mview_preprocessed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mview_preprocessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please do some preprocessing first.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed' is not defined"
     ]
    }
   ],
   "source": [
    "data.view_preprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca589b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073e47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fc23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBC:\n",
    "    def __init__(self, Data, n_dim, n_layers, \n",
    "                 optimizer = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08),\n",
    "                 loss_function = None, batch_size = 10, backend = \"default.qubit\", shots = 0):\n",
    "        \n",
    "        if loss_function == None:\n",
    "            self.loss_function = square_loss\n",
    "        else: \n",
    "            self.loss_function = loss_function\n",
    "        self.opt = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        dev = qml.device(backend, wires = self.n_dim, shots=shots)\n",
    "        #dev = qml.device('default.qubit.tf', wires = num_qubits, shots=1024)\n",
    "        #dev = qml.device('qiskit.ibmq', wires = num_qubits, backend='ibmq_manila', ibmqx_token=\"6cc75c58fc80fea56cb8dd391f8fbcfdb676a3dc7005493728bc9da7ea753e31a2110a01e3a0cc83f1a98f5ca79e32956fc66c11b5eea4cae163b3fa996be356\", shots=256)\n",
    "        #dev = qml.device('qiskit.basicaer', wires = num_qubits, shots = 256)\n",
    "\n",
    "        @qml.qnode(dev)\n",
    "        def circuit(parameters, data):\n",
    "            for i in range(num_qubits):\n",
    "                qml.Hadamard(wires = i)\n",
    "\n",
    "            AngleEmbedding(features = data, wires = range(num_qubits), rotation = 'Y')\n",
    "\n",
    "            qml.StronglyEntanglingLayers(weights = parameters, wires = range(num_qubits))\n",
    "\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.weights = 0.01 * np.random.randn(self.n_layers, self.n_dim, 3, requires_grad=True)\n",
    "        self.bias = np.array(0.0, requires_grad=True)\n",
    "        \n",
    "    def variational_classifier(weights, bias, x):\n",
    "        return circuit(weights, x) + bias\n",
    "    \n",
    "    def square_loss(labels, predictions):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            loss = loss + (l - p) ** 2\n",
    "\n",
    "        loss = loss / len(labels)\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < 1e-5:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def cost(weights, bias, X, Y):\n",
    "        predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "        return self.loss_function(Y, predictions)\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        wbest = 0\n",
    "        bbest = 0\n",
    "        abest = 0\n",
    "\n",
    "        for it in range(n_epochs):\n",
    "\n",
    "            # weights update by one optimizer step\n",
    "\n",
    "            batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "            X_batch = X[batch_index]\n",
    "            Y_batch = Y[batch_index]\n",
    "            weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "            if accuracy(Y, predictions) > abest:\n",
    "                wbest = weights\n",
    "                bbest = bias\n",
    "                abest = accuracy(Y, predictions)\n",
    "                print('New best')\n",
    "\n",
    "            acc = accuracy(Y, predictions)\n",
    "\n",
    "            print(\n",
    "                \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "                    it + 1, cost(weights, bias, X, Y), acc\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

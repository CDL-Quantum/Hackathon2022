{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f465949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pennylane import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pennylane as qml\n",
    "#from pennylane_qiskit import IBMQDevice\n",
    "#from pennylane_qiskit import BasicAerDevice\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fa6d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    def __init__(self, data, target, sample_size = 0, test_split = 0.3, seed = 10):\n",
    "        self.data = data\n",
    "        self.preprocess_done = None\n",
    "        \n",
    "        if sample_size == 0:\n",
    "            self.data_sample = data\n",
    "        else:\n",
    "            self.data_sample = data.sample(sample_size)\n",
    "            \n",
    "        self.train_set, self.test_set = train_test_split(self.data_sample, test_size=test_split, random_state=seed)\n",
    "        \n",
    "        self.y_train = self.train_set[[target]]\n",
    "        self.y_test = self.test_set[[target]]\n",
    "        \n",
    "        self.x_train = self.train_set.drop(target, axis=1)\n",
    "        self.x_test = self.test_set.drop(target, axis=1)\n",
    "        \n",
    "    def view_info(self):\n",
    "        print(self.data_sample.info())\n",
    "        if self.preprocess_done == None:\n",
    "            print(\"No preprocessing done yet.\")\n",
    "        else:\n",
    "            print(\"Preprocessing done via: \", self.preprocess_done)\n",
    "        return self.data_sample.describe()\n",
    "    \n",
    "    def get_preprocessed(self, to_show = False):\n",
    "        if self.preprocess_done == None:\n",
    "            print(\"Please do some preprocessing first.\")\n",
    "        else:\n",
    "            \n",
    "            if to_show:\n",
    "                print(\"Training Set and Labels: \")\n",
    "                print(self.train_X_preprocessed)\n",
    "                print(self.train_Y_preprocessed)\n",
    "\n",
    "                print(\"Test Set and Labels: \")\n",
    "                print(self.test_X_preprocessed)\n",
    "                print(self.test_Y_preprocessed)\n",
    "            \n",
    "            return self.train_X_preprocessed, self.train_Y_preprocessed, self.test_X_preprocessed, self.test_Y_preprocessed\n",
    "    \n",
    "    def perform_LDA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"LDA\"\n",
    "        print(\"Performing LDA...\")\n",
    "        \n",
    "        length = len(self.x_train.columns)\n",
    "        split_feature = int(length/n_dim)\n",
    "        features_train = []\n",
    "        features_test = []\n",
    "        \n",
    "        # Split Features (for Yaqi to change)\n",
    "        for i in range(n_dim):\n",
    "            new_set_train = self.x_train.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_train.append(new_set_train)\n",
    "            \n",
    "            new_set_test = self.x_test.iloc[:,(i*split_feature):((i+1)*split_feature)]\n",
    "            features_test.append(new_set_test)\n",
    "        \n",
    "        # Run the LDA\n",
    "        lda = LDA(n_components= (n_dim - 1))\n",
    "        features_lda_train = []\n",
    "        features_lda_test = []\n",
    "        \n",
    "        for i in range(n_dim):\n",
    "            features_lda_train_new = lda.fit_transform(features_train[i], self.y_train)\n",
    "            features_lda_train.append(pd.DataFrame(features_lda_train_new))\n",
    "            \n",
    "            features_lda_test_new = lda.fit_transform(features_test[i], self.y_test)\n",
    "            features_lda_test.append(pd.DataFrame(features_lda_test_new))\n",
    "        \n",
    "        x_train_data = features_lda_train[0]\n",
    "        x_test_data = features_lda_test[0]\n",
    "        \n",
    "        # Join the results together\n",
    "        for i in range(1, n_dim):\n",
    "            l_suffix = \"_\" + str(i)\n",
    "            r_suffix = \"_\" + str(i+1)\n",
    "            x_train_data = x_train_data.join(features_lda_train[i], lsuffix=l_suffix, rsuffix=r_suffix)\n",
    "            x_test_data = x_test_data.join(features_lda_test[i], lsuffix=l_suffix, rsuffix=r_suffix)\n",
    "        \n",
    "        # Normalize\n",
    "        std_scale_train = StandardScaler().fit(x_train_data)\n",
    "        x_train_data = std_scale_train.transform(x_train_data)\n",
    "        \n",
    "        std_scale_test = StandardScaler().fit(x_test_data)\n",
    "        x_test_data = std_scale_test.transform(x_test_data)\n",
    "            \n",
    "        # shift label from {0, 1} to {-1, 1}\n",
    "        self.train_X_preprocessed = np.array(x_train_data, requires_grad=False)\n",
    "        self.train_Y_preprocessed = np.array(self.y_train.values[:,0] * 2 - np.ones(len(self.y_train.values[:,0])), requires_grad = False)\n",
    "        \n",
    "        self.test_X_preprocessed = np.array(x_test_data, requires_grad=False)\n",
    "        self.test_Y_preprocessed = np.array(self.y_test.values[:,0] * 2 - np.ones(len(self.y_test.values[:,0])), requires_grad = False)\n",
    "        \n",
    "    def perform_PCA(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"PCA\"\n",
    "        print(\"Performing PCA...\")\n",
    "        \n",
    "        self.y_train.value_counts(normalize=True)*100\n",
    "        self.y_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        pca = PCA(n_components=n_dim, svd_solver='full')\n",
    "        pca.fit(self.x_train)\n",
    "        x_train_pca = pca.transform(self.x_train)\n",
    "        pca.fit(self.x_test)\n",
    "        x_test_pca = pca.transform(self.x_test)\n",
    "        \n",
    "        train_X_preprocessed = normalize(x_train_pca)\n",
    "        test_X_preprocessed = normalize(x_test_pca)\n",
    "        \n",
    "        self.train_Y_preprocessed = np.array(self.y_train.values[:,0] * 2 - np.ones(len(self.y_train.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        self.train_X_preprocessed = np.array(train_X_preprocessed, requires_grad=False)\n",
    "        \n",
    "        self.test_Y_preprocessed = np.array(self.y_test.values[:,0] * 2 - np.ones(len(self.y_test.values[:,0])), requires_grad = False)  # shift label from {0, 1} to {-1, 1}\n",
    "        self.test_X_preprocessed = np.array(test_X_preprocessed, requires_grad=False)\n",
    "        \n",
    "    def perform_normalize(self, n_dim = 2):\n",
    "        \n",
    "        self.preprocess_done = \"Normalize\"\n",
    "        print(\"Performing Normalize...\")\n",
    "        \n",
    "        self.y_train.value_counts(normalize=True)*100\n",
    "        self.y_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        self.x_train.value_counts(normalize=True)*100\n",
    "        self.x_test.value_counts(normalize=True)*100\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12676440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fraud_detection_bank_dataset.csv', sep=',')\n",
    "df = df.astype(float)\n",
    "df = df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb2bfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PrepareData(data = df,target = \"targets\", sample_size = 2000, test_split = 0.3, seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34f2e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 10272 to 19363\n",
      "Columns: 113 entries, col_0 to targets\n",
      "dtypes: float64(113)\n",
      "memory usage: 1.7 MB\n",
      "None\n",
      "No preprocessing done yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>col_103</th>\n",
       "      <th>col_104</th>\n",
       "      <th>col_105</th>\n",
       "      <th>col_106</th>\n",
       "      <th>col_107</th>\n",
       "      <th>col_108</th>\n",
       "      <th>col_109</th>\n",
       "      <th>col_110</th>\n",
       "      <th>col_111</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.013000</td>\n",
       "      <td>309.904500</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>2.378000</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>1.066000</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>3.013000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>47.011500</td>\n",
       "      <td>0.26400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.092556</td>\n",
       "      <td>893.336008</td>\n",
       "      <td>20.937306</td>\n",
       "      <td>8.773574</td>\n",
       "      <td>1.135793</td>\n",
       "      <td>6.593283</td>\n",
       "      <td>3.051894</td>\n",
       "      <td>14.092556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113514</td>\n",
       "      <td>0.487301</td>\n",
       "      <td>0.031615</td>\n",
       "      <td>0.465816</td>\n",
       "      <td>0.398212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219029</td>\n",
       "      <td>0.3007</td>\n",
       "      <td>64.113467</td>\n",
       "      <td>0.44091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>286.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>490.000000</td>\n",
       "      <td>25927.000000</td>\n",
       "      <td>904.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_0         col_1        col_2        col_3        col_4  \\\n",
       "count  2000.000000   2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      3.013000    309.904500     0.886500     2.378000     0.095500   \n",
       "std      14.092556    893.336008    20.937306     8.773574     1.135793   \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     37.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     97.500000     0.000000     1.000000     0.000000   \n",
       "75%       2.000000    286.250000     0.000000     2.000000     0.000000   \n",
       "max     490.000000  25927.000000   904.000000   235.000000    40.000000   \n",
       "\n",
       "             col_5        col_6        col_7   col_8   col_9  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.0  2000.0  ...   \n",
       "mean      1.066000     2.440000     3.013000     0.0     0.0  ...   \n",
       "std       6.593283     3.051894    14.092556     0.0     0.0  ...   \n",
       "min       0.000000    -1.000000     0.000000     0.0     0.0  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.0     0.0  ...   \n",
       "50%       0.000000     2.000000     0.000000     0.0     0.0  ...   \n",
       "75%       1.000000     6.000000     2.000000     0.0     0.0  ...   \n",
       "max     230.000000     8.000000   490.000000     0.0     0.0  ...   \n",
       "\n",
       "           col_103      col_104      col_105      col_106      col_107  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.011000     0.387500     0.001000     0.318000     0.197500   \n",
       "std       0.113514     0.487301     0.031615     0.465816     0.398212   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     1.000000     0.000000     1.000000     0.000000   \n",
       "max       2.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       col_108      col_109    col_110      col_111     targets  \n",
       "count   2000.0  2000.000000  2000.0000  2000.000000  2000.00000  \n",
       "mean       0.0     0.050500     0.0250    47.011500     0.26400  \n",
       "std        0.0     0.219029     0.3007    64.113467     0.44091  \n",
       "min        0.0     0.000000     0.0000     0.000000     0.00000  \n",
       "25%        0.0     0.000000     0.0000     6.000000     0.00000  \n",
       "50%        0.0     0.000000     0.0000    22.000000     0.00000  \n",
       "75%        0.0     0.000000     0.0000    62.000000     1.00000  \n",
       "max        0.0     1.000000    11.0000   728.000000     1.00000  \n",
       "\n",
       "[8 rows x 113 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b1f6a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA...\n"
     ]
    }
   ],
   "source": [
    "data.perform_LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c856f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set and Labels: \n",
      "[[-0.99360793  0.11288612]\n",
      " [-0.99992919 -0.01190007]\n",
      " [-0.99991751 -0.01284414]\n",
      " ...\n",
      " [-0.99991742 -0.01285143]\n",
      " [-0.49123299  0.87102821]\n",
      " [-0.99991403 -0.01311251]]\n",
      "[-1. -1. -1. ...  1. -1. -1.]\n",
      "Test Set and Labels: \n",
      "[[-0.99992372 -0.01235157]\n",
      " [-0.99988422 -0.01521681]\n",
      " [-0.99992729 -0.01205916]\n",
      " ...\n",
      " [ 0.99999839 -0.00179606]\n",
      " [-0.9999273  -0.01205789]\n",
      " [-0.99992564 -0.01219491]]\n",
      "[-1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1.\n",
      "  1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1.\n",
      "  1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      " -1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.\n",
      " -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1.\n",
      "  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.\n",
      "  1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.\n",
      " -1.  1. -1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
      " -1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1.\n",
      " -1.  1.  1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.99360793,  0.11288612],\n",
       "         [-0.99992919, -0.01190007],\n",
       "         [-0.99991751, -0.01284414],\n",
       "         ...,\n",
       "         [-0.99991742, -0.01285143],\n",
       "         [-0.49123299,  0.87102821],\n",
       "         [-0.99991403, -0.01311251]], requires_grad=False),\n",
       " tensor([-1., -1., -1., ...,  1., -1., -1.], requires_grad=False),\n",
       " tensor([[-0.99992372, -0.01235157],\n",
       "         [-0.99988422, -0.01521681],\n",
       "         [-0.99992729, -0.01205916],\n",
       "         ...,\n",
       "         [ 0.99999839, -0.00179606],\n",
       "         [-0.9999273 , -0.01205789],\n",
       "         [-0.99992564, -0.01219491]], requires_grad=False),\n",
       " tensor([-1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,\n",
       "         -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
       "         -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
       "         -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
       "         -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "         -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
       "         -1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
       "         -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "          1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
       "         -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
       "          1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
       "          1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         -1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "         -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "         -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
       "         -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "         -1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
       "         -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
       "         -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "          1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "          1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,\n",
       "          1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,\n",
       "          1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
       "         -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,\n",
       "         -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "          1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "         -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
       "         -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
       "         -1., -1.], requires_grad=False))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_preprocessed(to_show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef23506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ce4e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QBC:\n",
    "    def __init__(self, data, n_dim, n_layers, \n",
    "                 optimizer = AdamOptimizer(stepsize=0.1, beta1=0.9, beta2=0.99, eps=1e-08),\n",
    "                 interface_type = \"autograd\",\n",
    "                 loss_function = None,  backend = \"default.qubit\", shots = None):\n",
    "        \n",
    "        if loss_function == None:\n",
    "            def square_loss(labels, predictions):\n",
    "                loss = 0\n",
    "                for l, p in zip(labels, predictions):\n",
    "                    loss = loss + (l - p) ** 2\n",
    "\n",
    "                loss = loss / len(labels)\n",
    "                return loss\n",
    "            self.loss_function = square_loss\n",
    "        else: \n",
    "            self.loss_function = loss_function\n",
    "        self.opt = optimizer\n",
    "        self.data = data\n",
    "        \n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        dev = qml.device(backend, wires = self.n_dim, shots=shots)\n",
    "        #dev = qml.device('default.qubit.tf', wires = num_qubits, shots=1024)\n",
    "        #dev = qml.device('qiskit.ibmq', wires = num_qubits, backend='ibmq_manila', ibmqx_token=\"6cc75c58fc80fea56cb8dd391f8fbcfdb676a3dc7005493728bc9da7ea753e31a2110a01e3a0cc83f1a98f5ca79e32956fc66c11b5eea4cae163b3fa996be356\", shots=256)\n",
    "        #dev = qml.device('qiskit.basicaer', wires = num_qubits, shots = 256)\n",
    "\n",
    "        @qml.qnode(dev)\n",
    "        def circuit(parameters, data):\n",
    "            for i in range(n_dim):\n",
    "                qml.Hadamard(wires = i)\n",
    "\n",
    "            AngleEmbedding(features = data, wires = range(self.n_dim), rotation = 'Y')\n",
    "\n",
    "            qml.StronglyEntanglingLayers(weights = parameters, wires = range(self.n_dim))\n",
    "\n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.qlayer = qml.QNode(circuit, dev, interface=interface_type, diff_method='best')\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.weights = 0.01 * np.random.randn(self.n_layers, self.n_dim, 3, requires_grad=True)\n",
    "        self.bias = np.array(0.0, requires_grad=True)\n",
    "\n",
    "    def variational_classifier(self, weights, bias, x):\n",
    "        return self.qlayer(weights, x) + bias\n",
    "        \n",
    "    def train(self, batch_size = 10, n_epochs = 50):\n",
    "        wbest = 0\n",
    "        bbest = 0\n",
    "        abest = 0\n",
    "        X, Y, _, _ = self.data.get_preprocessed()\n",
    "        \n",
    "        def cost(weights, bias, X, Y):\n",
    "            predictions = [self.variational_classifier(weights, bias, x) for x in X]\n",
    "            return self.loss_function(Y, predictions)\n",
    "\n",
    "        def accuracy(labels, predictions):\n",
    "\n",
    "            loss = 0\n",
    "            for l, p in zip(labels, predictions):\n",
    "                if abs(l - p) < 1e-5:\n",
    "                    loss = loss + 1\n",
    "            loss = loss / len(labels)\n",
    "\n",
    "            return loss\n",
    "\n",
    "        for it in range(n_epochs):\n",
    "\n",
    "            # weights update by one optimizer step\n",
    "\n",
    "            batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "            X_batch = X[batch_index]\n",
    "            Y_batch = Y[batch_index]\n",
    "            self.weights, self.bias, _, _ = self.opt.step(cost, self.weights, self.bias, X_batch, Y_batch)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            predictions = [np.sign(self.variational_classifier(self.weights, self.bias, x)) for x in X]\n",
    "\n",
    "            if accuracy(Y, predictions) > abest:\n",
    "                wbest = self.weights\n",
    "                bbest = self.bias\n",
    "                abest = accuracy(Y, predictions)\n",
    "                print('New best')\n",
    "\n",
    "            acc = accuracy(Y, predictions)\n",
    "\n",
    "            print(\n",
    "                \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "                    it + 1, cost(self.weights, self.bias, X, Y), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.weights = wbest\n",
    "        self.bias = bbest\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        predictions = [np.sign(self.variational_classifier(self.weights, self.bias, x)) for x in test_data]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a63a4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QBC(data, n_dim = 2, n_layers = 5, backend = \"lightning.qubit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "37e6e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best\n",
      "Iter:     1 | Cost: 0.8362042 | Accuracy: 0.7278571 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/1676823408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/3881831539.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size, n_epochs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m# Compute the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariational_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mabest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/3881831539.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m# Compute the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariational_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mabest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39060/3881831539.py\u001b[0m in \u001b[0;36mvariational_classifier\u001b[1;34m(self, weights, bias, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvariational_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\pennylane\\qnode.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape_cached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musing_custom_cache\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         res = qml.execute(\n\u001b[0m\u001b[0;32m    579\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\pennylane\\interfaces\\batch\\__init__.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRUCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcachesize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetsizeof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mqml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mbatch_execute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_shots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride_shots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_execute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexpand_fn\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"device\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\functools.py\u001b[0m in \u001b[0;36mupdate_wrapper\u001b[1;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f442a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

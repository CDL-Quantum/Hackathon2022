{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daae30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation Imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit Imports\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Qiskit Imports\n",
    "from qiskit import Aer, execute\n",
    "from qiskit.circuit import QuantumCircuit, Parameter, ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit.circuit.library import TwoLocal, NLocal, RealAmplitudes, EfficientSU2\n",
    "from qiskit.circuit.library import HGate, RXGate, RYGate, RZGate, CXGate, CRXGate, CRZGate\n",
    "from qiskit.opflow import StateFn, PauliSumOp, AerPauliExpectation, ListOp, Gradient\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e25cdbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.63031449 2.93514704]\n"
     ]
    }
   ],
   "source": [
    "# Generate random inputs\n",
    "inputs = np.random.uniform(low=0.0, high=np.pi, size=2)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8001eaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 568.052x144.48 with 1 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode random vector into a circuit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "inputs = np.random.uniform(low=0.0, high=np.pi, size=2)\n",
    "\n",
    "\n",
    "def encode_map(input_data, dim = 2):\n",
    "    \n",
    "    q_num = len(input_data)\n",
    "    encode_map = ZZFeatureMap(feature_dimension=2, reps=1, insert_barriers=True)\n",
    "    #encode_circuit = encode_map.bind_parameters(input_data)\n",
    "    \n",
    "    return encode_map\n",
    "\n",
    "\n",
    "feature_map = encode_map(inputs)\n",
    "\n",
    "feature_map.decompose().draw(output='mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10f3cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌───┐     ┌───┐\n",
      "q_0: ┤ H ├──■──┤ H ├\n",
      "     └───┘┌─┴─┐└───┘\n",
      "q_1: ─────┤ X ├─────\n",
      "          └───┘     \n",
      "     ┌──────────┐┌──────────┐     ┌──────────┐┌──────────┐\n",
      "q_0: ┤ Ry(θ[0]) ├┤ Rx(θ[2]) ├──■──┤ Ry(θ[4]) ├┤ Rx(θ[6]) ├\n",
      "     ├──────────┤├──────────┤┌─┴─┐├──────────┤├──────────┤\n",
      "q_1: ┤ Ry(θ[1]) ├┤ Rx(θ[3]) ├┤ X ├┤ Ry(θ[5]) ├┤ Rx(θ[7]) ├\n",
      "     └──────────┘└──────────┘└───┘└──────────┘└──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Construct teacher and student networks\n",
    "\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "\n",
    "def teacher_circuit_builder(num_qubits):\n",
    "    circuit = QuantumCircuit(num_qubits)\n",
    "\n",
    "    circuit.h(0)\n",
    "    circuit.cx(0, 1)\n",
    "    circuit.h(0)\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "\n",
    "def student_network_builder(num_qubits):\n",
    "    circuit = TwoLocal(num_qubits=num_qubits, reps=1, entanglement_blocks='cx', entanglement='linear', rotation_blocks=['ry','rx']).decompose()\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "teacher_circuit = teacher_circuit_builder(2)\n",
    "print(teacher_circuit)\n",
    "\n",
    "student_network = student_network_builder(2)\n",
    "print(student_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85fcabe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterView([ParameterVectorElement(x[0]), ParameterVectorElement(x[1]), ParameterVectorElement(θ[0]), ParameterVectorElement(θ[1]), ParameterVectorElement(θ[2]), ParameterVectorElement(θ[3]), ParameterVectorElement(θ[4]), ParameterVectorElement(θ[5]), ParameterVectorElement(θ[6]), ParameterVectorElement(θ[7])])\n",
      "ParameterView([])\n"
     ]
    }
   ],
   "source": [
    "# Glue together feature map and network\n",
    "\n",
    "\n",
    "tc_full = feature_map.compose(teacher_circuit)\n",
    "\n",
    "sc_full = feature_map.compose(student_network)\n",
    "\n",
    "\n",
    "print(sc_full.parameters)\n",
    "print(teacher_circuit.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87421beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up backed\n",
    "\n",
    "from qiskit import Aer\n",
    "from qiskit.utils import QuantumInstance\n",
    "\n",
    "backend = Aer.get_backend('qasm_simulator')\n",
    "q_instance = QuantumInstance(backend, shots = 8192, seed_simulator = 2718, seed_transpiler = 2718)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4defc9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24731445 0.2442627  0.25170898 0.25671387]\n",
      " [0.24279785 0.25610352 0.24731445 0.25378418]]\n",
      "[[0.24731445 0.2442627  0.25170898 0.25671387]\n",
      " [0.3359375  0.046875   0.04370117 0.57348633]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#  QNN formulation\n",
    "\n",
    "\n",
    "qnn_student = CircuitQNN(sc_full, feature_map.parameters, student_network.parameters, quantum_instance=q_instance)\n",
    "qnn_teacher = CircuitQNN(tc_full, feature_map.parameters, teacher_circuit.parameters, quantum_instance=q_instance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data = [[0., 0.],[1., 1.] ]\n",
    "\n",
    "weights = [0. , 0., 0., 0.,0.,0.,0.,0.]\n",
    "\n",
    "print(qnn_student.forward(input_data, weights))\n",
    "print(qnn_teacher.forward(input_data, []))\n",
    "print(qnn_student.num_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62107b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00741414  0.03692014 -0.09518764 -0.0651257   0.08205347 -0.06788118\n",
      " -0.06402362 -0.09932174]\n",
      "tensor([[0.1849, 0.2319, 0.2657, 0.3174],\n",
      "        [0.3367, 0.1996, 0.1998, 0.2639]], grad_fn=<_TorchNNFunctionBackward>)\n",
      "tensor([[0.2473, 0.2443, 0.2517, 0.2567],\n",
      "        [0.3359, 0.0469, 0.0437, 0.5735]], grad_fn=<_TorchNNFunctionBackward>)\n",
      "tensor(1.3710, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Defining a PyTorch model and test it\n",
    "\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss, KLDivLoss\n",
    "\n",
    "initial_weights = 0.1 * (2 * algorithm_globals.random.random(qnn_student.num_weights) - 1)\n",
    "print(initial_weights)\n",
    "#initial_weights = [0. , 0., 0., 0.]\n",
    "student_model = TorchConnector(qnn_student, initial_weights=initial_weights)\n",
    "teacher_model = TorchConnector(qnn_teacher, initial_weights=[])\n",
    "\n",
    "\n",
    "# test on a dummy input\n",
    "input_data = [[0., 0.],[1., 1.]]\n",
    "\n",
    "f_loss = MSELoss(reduction=\"sum\")\n",
    "\n",
    "ce_loss = CrossEntropyLoss() \n",
    "\n",
    "output = student_model(Tensor(input_data))  # Forward pass\n",
    "target = teacher_model(Tensor(input_data))\n",
    "\n",
    "\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(target)\n",
    "\n",
    "\n",
    "print(ce_loss(output, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cc6dc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.08\n",
      "MSE Loss 0.0311 CE_Loss: 1.3848 KL_Loss: -0.3959\n",
      "MSE Loss 0.2558 CE_Loss: 1.3963 KL_Loss: -0.2988\n",
      "MSE Loss 0.3340 CE_Loss: 1.4015 KL_Loss: -0.2723\n",
      "MSE Loss 0.1067 CE_Loss: 1.3237 KL_Loss: -0.3174\n",
      "MSE Loss 0.0232 CE_Loss: 1.3835 KL_Loss: -0.4001\n",
      "Training [1%]\tLoss: 0.1639\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0357 CE_Loss: 1.3542 KL_Loss: -0.3711\n",
      "MSE Loss 0.6381 CE_Loss: 1.4905 KL_Loss: -0.2316\n",
      "MSE Loss 0.0278 CE_Loss: 1.3829 KL_Loss: -0.4082\n",
      "MSE Loss 0.4380 CE_Loss: 1.4024 KL_Loss: -0.2308\n",
      "MSE Loss 0.1135 CE_Loss: 1.3089 KL_Loss: -0.3160\n",
      "Training [2%]\tLoss: 0.2645\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.1228 CE_Loss: 1.4165 KL_Loss: -0.3935\n",
      "MSE Loss 0.1494 CE_Loss: 1.3475 KL_Loss: -0.2907\n",
      "MSE Loss 0.0342 CE_Loss: 1.3604 KL_Loss: -0.3997\n",
      "MSE Loss 0.1221 CE_Loss: 1.3437 KL_Loss: -0.3541\n",
      "MSE Loss 0.1616 CE_Loss: 1.4254 KL_Loss: -0.3924\n",
      "Training [3%]\tLoss: 0.1318\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.1595 CE_Loss: 1.3297 KL_Loss: -0.3196\n",
      "MSE Loss 0.1961 CE_Loss: 1.2300 KL_Loss: -0.2498\n",
      "MSE Loss 0.0661 CE_Loss: 1.3418 KL_Loss: -0.3803\n",
      "MSE Loss 0.0287 CE_Loss: 1.3039 KL_Loss: -0.3720\n",
      "MSE Loss 0.5708 CE_Loss: 1.4483 KL_Loss: -0.2188\n",
      "Training [4%]\tLoss: 0.2175\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0535 CE_Loss: 1.3486 KL_Loss: -0.3828\n",
      "MSE Loss 0.2326 CE_Loss: 1.3905 KL_Loss: -0.2994\n",
      "MSE Loss 0.1987 CE_Loss: 1.4400 KL_Loss: -0.3844\n",
      "MSE Loss 0.3721 CE_Loss: 1.3675 KL_Loss: -0.2493\n",
      "MSE Loss 0.1323 CE_Loss: 1.4183 KL_Loss: -0.3823\n",
      "Training [5%]\tLoss: 0.2118\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0341 CE_Loss: 1.2604 KL_Loss: -0.3429\n",
      "MSE Loss 0.1473 CE_Loss: 1.4221 KL_Loss: -0.3841\n",
      "MSE Loss 0.3715 CE_Loss: 1.2861 KL_Loss: -0.1881\n",
      "MSE Loss 0.0069 CE_Loss: 1.3084 KL_Loss: -0.3948\n",
      "MSE Loss 0.0589 CE_Loss: 1.3907 KL_Loss: -0.3974\n",
      "Training [6%]\tLoss: 0.1370\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0891 CE_Loss: 1.3033 KL_Loss: -0.3345\n",
      "MSE Loss 0.2098 CE_Loss: 1.4390 KL_Loss: -0.3931\n",
      "MSE Loss 0.0457 CE_Loss: 1.3807 KL_Loss: -0.3873\n",
      "MSE Loss 0.1983 CE_Loss: 1.4114 KL_Loss: -0.4089\n",
      "MSE Loss 0.0826 CE_Loss: 1.0733 KL_Loss: -0.2718\n",
      "Training [7%]\tLoss: 0.1383\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0125 CE_Loss: 1.1506 KL_Loss: -0.3321\n",
      "MSE Loss 0.0462 CE_Loss: 1.0881 KL_Loss: -0.2732\n",
      "MSE Loss 0.1678 CE_Loss: 1.3413 KL_Loss: -0.3976\n",
      "MSE Loss 0.0072 CE_Loss: 1.2416 KL_Loss: -0.3408\n",
      "MSE Loss 0.0481 CE_Loss: 1.0202 KL_Loss: -0.2766\n",
      "Training [8%]\tLoss: 0.0680\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0736 CE_Loss: 1.3429 KL_Loss: -0.3612\n",
      "MSE Loss 0.0059 CE_Loss: 1.1372 KL_Loss: -0.3548\n",
      "MSE Loss 0.0027 CE_Loss: 1.2079 KL_Loss: -0.3723\n",
      "MSE Loss 0.1907 CE_Loss: 1.4145 KL_Loss: -0.4073\n",
      "MSE Loss 0.0072 CE_Loss: 1.2558 KL_Loss: -0.3877\n",
      "Training [9%]\tLoss: 0.0687\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0160 CE_Loss: 1.0970 KL_Loss: -0.2829\n",
      "MSE Loss 0.1431 CE_Loss: 1.3721 KL_Loss: -0.3993\n",
      "MSE Loss 0.0774 CE_Loss: 1.3505 KL_Loss: -0.3516\n",
      "MSE Loss 0.0248 CE_Loss: 1.2306 KL_Loss: -0.3455\n",
      "MSE Loss 0.0236 CE_Loss: 1.2104 KL_Loss: -0.3324\n",
      "Training [10%]\tLoss: 0.0695\n",
      "Learning rate: 0.04\n",
      "MSE Loss 0.0059 CE_Loss: 1.1214 KL_Loss: -0.3352\n",
      "MSE Loss 0.0051 CE_Loss: 1.0371 KL_Loss: -0.3068\n",
      "MSE Loss 0.0328 CE_Loss: 1.2376 KL_Loss: -0.3819\n",
      "MSE Loss 0.0009 CE_Loss: 1.2514 KL_Loss: -0.3667\n",
      "MSE Loss 0.0107 CE_Loss: 1.3691 KL_Loss: -0.3987\n",
      "Training [11%]\tLoss: 0.0231\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0345 CE_Loss: 1.3699 KL_Loss: -0.4056\n",
      "MSE Loss 0.0004 CE_Loss: 0.8610 KL_Loss: -0.2883\n",
      "MSE Loss 0.0206 CE_Loss: 1.3731 KL_Loss: -0.4090\n",
      "MSE Loss 0.0079 CE_Loss: 1.3107 KL_Loss: -0.3978\n",
      "MSE Loss 0.0090 CE_Loss: 1.1266 KL_Loss: -0.3361\n",
      "Training [12%]\tLoss: 0.0266\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0008 CE_Loss: 1.1803 KL_Loss: -0.3538\n",
      "MSE Loss 0.0019 CE_Loss: 1.3065 KL_Loss: -0.3884\n",
      "MSE Loss 0.0341 CE_Loss: 1.1799 KL_Loss: -0.3122\n",
      "MSE Loss 0.0241 CE_Loss: 1.2102 KL_Loss: -0.3338\n",
      "MSE Loss 0.0013 CE_Loss: 1.0444 KL_Loss: -0.3468\n",
      "Training [13%]\tLoss: 0.0243\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0005 CE_Loss: 1.1514 KL_Loss: -0.3571\n",
      "MSE Loss 0.0017 CE_Loss: 1.0200 KL_Loss: -0.3303\n",
      "MSE Loss 0.0140 CE_Loss: 1.3860 KL_Loss: -0.4092\n",
      "MSE Loss 0.0020 CE_Loss: 1.0606 KL_Loss: -0.3237\n",
      "MSE Loss 0.0024 CE_Loss: 1.3647 KL_Loss: -0.4040\n",
      "Training [14%]\tLoss: 0.0161\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0095 CE_Loss: 1.0900 KL_Loss: -0.3112\n",
      "MSE Loss 0.0158 CE_Loss: 1.3454 KL_Loss: -0.3995\n",
      "MSE Loss 0.0005 CE_Loss: 1.1372 KL_Loss: -0.3565\n",
      "MSE Loss 0.0113 CE_Loss: 1.1505 KL_Loss: -0.3393\n",
      "MSE Loss 0.0096 CE_Loss: 1.1577 KL_Loss: -0.3257\n",
      "Training [15%]\tLoss: 0.0211\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0000 CE_Loss: 0.9426 KL_Loss: -0.3182\n",
      "MSE Loss 0.0001 CE_Loss: 1.1412 KL_Loss: -0.3505\n",
      "MSE Loss 0.0000 CE_Loss: 0.9011 KL_Loss: -0.2987\n",
      "MSE Loss 0.0000 CE_Loss: 1.2293 KL_Loss: -0.3493\n",
      "MSE Loss 0.0008 CE_Loss: 1.0369 KL_Loss: -0.3321\n",
      "Training [16%]\tLoss: 0.0107\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0000 CE_Loss: 1.3850 KL_Loss: -0.4087\n",
      "MSE Loss 0.0024 CE_Loss: 1.3798 KL_Loss: -0.4071\n",
      "MSE Loss 0.0001 CE_Loss: 1.1828 KL_Loss: -0.3603\n",
      "MSE Loss 0.0004 CE_Loss: 1.1712 KL_Loss: -0.3115\n",
      "MSE Loss 0.0001 CE_Loss: 1.2251 KL_Loss: -0.3466\n",
      "Training [17%]\tLoss: 0.0133\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0002 CE_Loss: 0.7854 KL_Loss: -0.2729\n",
      "MSE Loss 0.0014 CE_Loss: 1.3726 KL_Loss: -0.4038\n",
      "MSE Loss 0.0006 CE_Loss: 1.1237 KL_Loss: -0.3584\n",
      "MSE Loss 0.0057 CE_Loss: 1.1668 KL_Loss: -0.3440\n",
      "MSE Loss 0.0007 CE_Loss: 1.3820 KL_Loss: -0.4081\n",
      "Training [18%]\tLoss: 0.0134\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0026 CE_Loss: 1.2739 KL_Loss: -0.3854\n",
      "MSE Loss 0.0009 CE_Loss: 1.2591 KL_Loss: -0.3870\n",
      "MSE Loss 0.0019 CE_Loss: 1.3215 KL_Loss: -0.3859\n",
      "MSE Loss 0.0004 CE_Loss: 1.1446 KL_Loss: -0.3418\n",
      "MSE Loss 0.0024 CE_Loss: 1.2922 KL_Loss: -0.3901\n",
      "Training [19%]\tLoss: 0.0142\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0017 CE_Loss: 1.3833 KL_Loss: -0.4086\n",
      "MSE Loss 0.0008 CE_Loss: 1.3675 KL_Loss: -0.4040\n",
      "MSE Loss 0.0000 CE_Loss: 0.7730 KL_Loss: -0.2654\n",
      "MSE Loss 0.0004 CE_Loss: 1.2045 KL_Loss: -0.3538\n",
      "MSE Loss 0.0001 CE_Loss: 1.0268 KL_Loss: -0.3379\n",
      "Training [20%]\tLoss: 0.0121\n",
      "Learning rate: 0.02\n",
      "MSE Loss 0.0001 CE_Loss: 1.1143 KL_Loss: -0.3551\n",
      "MSE Loss 0.0001 CE_Loss: 1.0285 KL_Loss: -0.3275\n",
      "MSE Loss 0.0018 CE_Loss: 1.3628 KL_Loss: -0.4027\n",
      "MSE Loss 0.0035 CE_Loss: 1.3262 KL_Loss: -0.3904\n",
      "MSE Loss 0.0010 CE_Loss: 1.0926 KL_Loss: -0.3473\n",
      "Training [21%]\tLoss: 0.0131\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0005 CE_Loss: 1.3611 KL_Loss: -0.4012\n",
      "MSE Loss 0.0005 CE_Loss: 1.2417 KL_Loss: -0.3707\n",
      "MSE Loss 0.0001 CE_Loss: 1.2798 KL_Loss: -0.3839\n",
      "MSE Loss 0.0006 CE_Loss: 1.3617 KL_Loss: -0.4027\n",
      "MSE Loss 0.0001 CE_Loss: 1.2290 KL_Loss: -0.3605\n",
      "Training [22%]\tLoss: 0.0133\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0000 CE_Loss: 1.0630 KL_Loss: -0.3375\n",
      "MSE Loss 0.0000 CE_Loss: 1.2830 KL_Loss: -0.3848\n",
      "MSE Loss 0.0002 CE_Loss: 1.3154 KL_Loss: -0.3871\n",
      "MSE Loss 0.0000 CE_Loss: 0.9596 KL_Loss: -0.3161\n",
      "MSE Loss 0.0002 CE_Loss: 1.3055 KL_Loss: -0.3900\n",
      "Training [23%]\tLoss: 0.0119\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0010 CE_Loss: 1.2701 KL_Loss: -0.3796\n",
      "MSE Loss 0.0000 CE_Loss: 0.8784 KL_Loss: -0.3004\n",
      "MSE Loss 0.0002 CE_Loss: 1.1326 KL_Loss: -0.3580\n",
      "MSE Loss 0.0001 CE_Loss: 1.0747 KL_Loss: -0.3397\n",
      "MSE Loss 0.0003 CE_Loss: 1.1413 KL_Loss: -0.3629\n",
      "Training [24%]\tLoss: 0.0113\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0000 CE_Loss: 1.0630 KL_Loss: -0.3416\n",
      "MSE Loss 0.0015 CE_Loss: 1.1027 KL_Loss: -0.3233\n",
      "MSE Loss 0.0011 CE_Loss: 1.3784 KL_Loss: -0.4055\n",
      "MSE Loss 0.0002 CE_Loss: 1.3721 KL_Loss: -0.4052\n",
      "MSE Loss 0.0001 CE_Loss: 1.2984 KL_Loss: -0.3888\n",
      "Training [25%]\tLoss: 0.0130\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0001 CE_Loss: 1.3738 KL_Loss: -0.4058\n",
      "MSE Loss 0.0001 CE_Loss: 1.0897 KL_Loss: -0.3521\n",
      "MSE Loss 0.0000 CE_Loss: 1.2752 KL_Loss: -0.3769\n",
      "MSE Loss 0.0001 CE_Loss: 1.3059 KL_Loss: -0.3856\n",
      "MSE Loss 0.0000 CE_Loss: 1.3217 KL_Loss: -0.3946\n",
      "Training [26%]\tLoss: 0.0128\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0001 CE_Loss: 1.1988 KL_Loss: -0.3622\n",
      "MSE Loss 0.0001 CE_Loss: 1.3817 KL_Loss: -0.4076\n",
      "MSE Loss 0.0002 CE_Loss: 1.3069 KL_Loss: -0.3861\n",
      "MSE Loss 0.0001 CE_Loss: 1.1818 KL_Loss: -0.3175\n",
      "MSE Loss 0.0002 CE_Loss: 0.8344 KL_Loss: -0.2741\n",
      "Training [27%]\tLoss: 0.0119\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0001 CE_Loss: 1.2118 KL_Loss: -0.3698\n",
      "MSE Loss 0.0001 CE_Loss: 1.3327 KL_Loss: -0.3919\n",
      "MSE Loss 0.0000 CE_Loss: 1.1136 KL_Loss: -0.3318\n",
      "MSE Loss 0.0000 CE_Loss: 1.0819 KL_Loss: -0.3324\n",
      "MSE Loss 0.0000 CE_Loss: 1.3623 KL_Loss: -0.4028\n",
      "Training [28%]\tLoss: 0.0122\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0001 CE_Loss: 1.3553 KL_Loss: -0.3994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss 0.0000 CE_Loss: 1.3538 KL_Loss: -0.3999\n",
      "MSE Loss 0.0000 CE_Loss: 1.3855 KL_Loss: -0.4089\n",
      "MSE Loss 0.0000 CE_Loss: 1.1624 KL_Loss: -0.3386\n",
      "MSE Loss 0.0000 CE_Loss: 1.3630 KL_Loss: -0.4025\n",
      "Training [29%]\tLoss: 0.0133\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0001 CE_Loss: 1.3647 KL_Loss: -0.4025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3490 KL_Loss: -0.3979\n",
      "MSE Loss 0.0000 CE_Loss: 1.3685 KL_Loss: -0.4042\n",
      "MSE Loss 0.0000 CE_Loss: 1.3805 KL_Loss: -0.4076\n",
      "MSE Loss 0.0000 CE_Loss: 1.3254 KL_Loss: -0.3957\n",
      "Training [30%]\tLoss: 0.0136\n",
      "Learning rate: 0.01\n",
      "MSE Loss 0.0000 CE_Loss: 1.3385 KL_Loss: -0.3952\n",
      "MSE Loss 0.0000 CE_Loss: 1.3778 KL_Loss: -0.4065\n",
      "MSE Loss 0.0000 CE_Loss: 1.0558 KL_Loss: -0.3370\n",
      "MSE Loss 0.0000 CE_Loss: 1.2610 KL_Loss: -0.3784\n",
      "MSE Loss 0.0000 CE_Loss: 1.3590 KL_Loss: -0.4013\n",
      "Training [31%]\tLoss: 0.0128\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 0.9554 KL_Loss: -0.3116\n",
      "MSE Loss 0.0000 CE_Loss: 1.1313 KL_Loss: -0.3326\n",
      "MSE Loss 0.0000 CE_Loss: 1.0811 KL_Loss: -0.3461\n",
      "MSE Loss 0.0000 CE_Loss: 1.0993 KL_Loss: -0.3209\n",
      "MSE Loss 0.0000 CE_Loss: 1.2216 KL_Loss: -0.3695\n",
      "Training [32%]\tLoss: 0.0110\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.2795 KL_Loss: -0.3848\n",
      "MSE Loss 0.0000 CE_Loss: 1.2414 KL_Loss: -0.3550\n",
      "MSE Loss 0.0000 CE_Loss: 1.3090 KL_Loss: -0.3931\n",
      "MSE Loss 0.0000 CE_Loss: 0.9696 KL_Loss: -0.3137\n",
      "MSE Loss 0.0000 CE_Loss: 1.1080 KL_Loss: -0.3500\n",
      "Training [33%]\tLoss: 0.0118\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.0171 KL_Loss: -0.3306\n",
      "MSE Loss 0.0000 CE_Loss: 0.8638 KL_Loss: -0.2918\n",
      "MSE Loss 0.0000 CE_Loss: 1.3124 KL_Loss: -0.3901\n",
      "MSE Loss 0.0000 CE_Loss: 1.1331 KL_Loss: -0.3382\n",
      "MSE Loss 0.0000 CE_Loss: 1.3420 KL_Loss: -0.3962\n",
      "Training [34%]\tLoss: 0.0113\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0001 CE_Loss: 1.2668 KL_Loss: -0.3822\n",
      "MSE Loss 0.0000 CE_Loss: 1.3678 KL_Loss: -0.4039\n",
      "MSE Loss 0.0000 CE_Loss: 1.2796 KL_Loss: -0.3830\n",
      "MSE Loss 0.0000 CE_Loss: 1.1762 KL_Loss: -0.3111\n",
      "MSE Loss 0.0000 CE_Loss: 1.2137 KL_Loss: -0.3577\n",
      "Training [35%]\tLoss: 0.0126\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.1966 KL_Loss: -0.3677\n",
      "MSE Loss 0.0000 CE_Loss: 0.8217 KL_Loss: -0.2877\n",
      "MSE Loss 0.0000 CE_Loss: 1.0357 KL_Loss: -0.3418\n",
      "MSE Loss 0.0000 CE_Loss: 1.1706 KL_Loss: -0.3333\n",
      "MSE Loss 0.0000 CE_Loss: 1.3253 KL_Loss: -0.3922\n",
      "Training [36%]\tLoss: 0.0111\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.3775 KL_Loss: -0.4065\n",
      "MSE Loss 0.0000 CE_Loss: 1.1742 KL_Loss: -0.3094\n",
      "MSE Loss 0.0000 CE_Loss: 1.2891 KL_Loss: -0.3764\n",
      "MSE Loss 0.0000 CE_Loss: 1.2476 KL_Loss: -0.3630\n",
      "MSE Loss 0.0000 CE_Loss: 1.2693 KL_Loss: -0.3793\n",
      "Training [37%]\tLoss: 0.0127\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.1629 KL_Loss: -0.3486\n",
      "MSE Loss 0.0000 CE_Loss: 1.2412 KL_Loss: -0.3660\n",
      "MSE Loss 0.0001 CE_Loss: 1.2865 KL_Loss: -0.3751\n",
      "MSE Loss 0.0000 CE_Loss: 1.3151 KL_Loss: -0.3880\n",
      "MSE Loss 0.0000 CE_Loss: 1.0613 KL_Loss: -0.3378\n",
      "Training [38%]\tLoss: 0.0122\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.2506 KL_Loss: -0.3765\n",
      "MSE Loss 0.0000 CE_Loss: 0.9980 KL_Loss: -0.3073\n",
      "MSE Loss 0.0000 CE_Loss: 1.3451 KL_Loss: -0.3964\n",
      "MSE Loss 0.0000 CE_Loss: 0.9386 KL_Loss: -0.3208\n",
      "MSE Loss 0.0000 CE_Loss: 1.3186 KL_Loss: -0.3898\n",
      "Training [39%]\tLoss: 0.0117\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.2000 KL_Loss: -0.3727\n",
      "MSE Loss 0.0000 CE_Loss: 0.9171 KL_Loss: -0.2953\n",
      "MSE Loss 0.0000 CE_Loss: 1.0641 KL_Loss: -0.3451\n",
      "MSE Loss 0.0000 CE_Loss: 0.9906 KL_Loss: -0.3116\n",
      "MSE Loss 0.0000 CE_Loss: 1.3786 KL_Loss: -0.4069\n",
      "Training [40%]\tLoss: 0.0111\n",
      "Learning rate: 0.005\n",
      "MSE Loss 0.0000 CE_Loss: 1.3719 KL_Loss: -0.4049\n",
      "MSE Loss 0.0000 CE_Loss: 1.3556 KL_Loss: -0.4001\n",
      "MSE Loss 0.0000 CE_Loss: 1.0596 KL_Loss: -0.3091\n",
      "MSE Loss 0.0000 CE_Loss: 0.8691 KL_Loss: -0.2971\n",
      "MSE Loss 0.0000 CE_Loss: 0.8885 KL_Loss: -0.3033\n",
      "Training [41%]\tLoss: 0.0111\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3523 KL_Loss: -0.4006\n",
      "MSE Loss 0.0000 CE_Loss: 0.9881 KL_Loss: -0.3250\n",
      "MSE Loss 0.0000 CE_Loss: 1.2173 KL_Loss: -0.3710\n",
      "MSE Loss 0.0000 CE_Loss: 1.3621 KL_Loss: -0.4034\n",
      "MSE Loss 0.0000 CE_Loss: 1.3467 KL_Loss: -0.3988\n",
      "Training [42%]\tLoss: 0.0125\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.1149 KL_Loss: -0.3319\n",
      "MSE Loss 0.0000 CE_Loss: 1.3516 KL_Loss: -0.3999\n",
      "MSE Loss 0.0000 CE_Loss: 1.2095 KL_Loss: -0.3491\n",
      "MSE Loss 0.0000 CE_Loss: 1.2767 KL_Loss: -0.3726\n",
      "MSE Loss 0.0000 CE_Loss: 1.3181 KL_Loss: -0.3926\n",
      "Training [43%]\tLoss: 0.0125\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3117 KL_Loss: -0.3850\n",
      "MSE Loss 0.0000 CE_Loss: 1.2652 KL_Loss: -0.3737\n",
      "MSE Loss 0.0000 CE_Loss: 1.3733 KL_Loss: -0.4053\n",
      "MSE Loss 0.0000 CE_Loss: 1.1795 KL_Loss: -0.3645\n",
      "MSE Loss 0.0000 CE_Loss: 1.3348 KL_Loss: -0.3967\n",
      "Training [44%]\tLoss: 0.0129\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3674 KL_Loss: -0.4044\n",
      "MSE Loss 0.0000 CE_Loss: 1.2406 KL_Loss: -0.3769\n",
      "MSE Loss 0.0000 CE_Loss: 1.3840 KL_Loss: -0.4083\n",
      "MSE Loss 0.0000 CE_Loss: 1.0483 KL_Loss: -0.3414\n",
      "MSE Loss 0.0000 CE_Loss: 1.2499 KL_Loss: -0.3678\n",
      "Training [45%]\tLoss: 0.0126\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.2048 KL_Loss: -0.3354\n",
      "MSE Loss 0.0000 CE_Loss: 1.0160 KL_Loss: -0.3068\n",
      "MSE Loss 0.0000 CE_Loss: 1.2649 KL_Loss: -0.3681\n",
      "MSE Loss 0.0000 CE_Loss: 0.9835 KL_Loss: -0.3047\n",
      "MSE Loss 0.0000 CE_Loss: 1.3267 KL_Loss: -0.3947\n",
      "Training [46%]\tLoss: 0.0116\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.2146 KL_Loss: -0.3593\n",
      "MSE Loss 0.0000 CE_Loss: 1.3290 KL_Loss: -0.3944\n",
      "MSE Loss 0.0000 CE_Loss: 1.1608 KL_Loss: -0.3583\n",
      "MSE Loss 0.0000 CE_Loss: 1.3841 KL_Loss: -0.4085\n",
      "MSE Loss 0.0000 CE_Loss: 1.1273 KL_Loss: -0.3502\n",
      "Training [47%]\tLoss: 0.0124\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3746 KL_Loss: -0.4059\n",
      "MSE Loss 0.0000 CE_Loss: 1.3661 KL_Loss: -0.4037\n",
      "MSE Loss 0.0000 CE_Loss: 1.3394 KL_Loss: -0.3948\n",
      "MSE Loss 0.0000 CE_Loss: 1.0990 KL_Loss: -0.3261\n",
      "MSE Loss 0.0000 CE_Loss: 0.9276 KL_Loss: -0.2964\n",
      "Training [48%]\tLoss: 0.0122\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.2919 KL_Loss: -0.3855\n",
      "MSE Loss 0.0000 CE_Loss: 1.2107 KL_Loss: -0.3442\n",
      "MSE Loss 0.0000 CE_Loss: 1.2785 KL_Loss: -0.3828\n",
      "MSE Loss 0.0000 CE_Loss: 1.2519 KL_Loss: -0.3727\n",
      "MSE Loss 0.0000 CE_Loss: 1.3826 KL_Loss: -0.4080\n",
      "Training [49%]\tLoss: 0.0128\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3687 KL_Loss: -0.4039\n",
      "MSE Loss 0.0000 CE_Loss: 1.1874 KL_Loss: -0.3680\n",
      "MSE Loss 0.0000 CE_Loss: 1.0403 KL_Loss: -0.3278\n",
      "MSE Loss 0.0000 CE_Loss: 1.1648 KL_Loss: -0.3056\n",
      "MSE Loss 0.0000 CE_Loss: 0.8093 KL_Loss: -0.2812\n",
      "Training [50%]\tLoss: 0.0111\n",
      "Learning rate: 0.0025\n",
      "MSE Loss 0.0000 CE_Loss: 1.3559 KL_Loss: -0.4016\n",
      "MSE Loss 0.0000 CE_Loss: 1.2304 KL_Loss: -0.3612\n",
      "MSE Loss 0.0000 CE_Loss: 1.3782 KL_Loss: -0.4067\n",
      "MSE Loss 0.0000 CE_Loss: 1.1528 KL_Loss: -0.3648\n",
      "MSE Loss 0.0000 CE_Loss: 1.3007 KL_Loss: -0.3869\n",
      "Training [51%]\tLoss: 0.0128\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3400 KL_Loss: -0.3961\n",
      "MSE Loss 0.0000 CE_Loss: 1.3542 KL_Loss: -0.4002\n",
      "MSE Loss 0.0000 CE_Loss: 1.1895 KL_Loss: -0.3327\n",
      "MSE Loss 0.0000 CE_Loss: 1.0775 KL_Loss: -0.3276\n",
      "MSE Loss 0.0000 CE_Loss: 1.0820 KL_Loss: -0.3136\n",
      "Training [52%]\tLoss: 0.0121\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2150 KL_Loss: -0.3442\n",
      "MSE Loss 0.0000 CE_Loss: 1.3850 KL_Loss: -0.4087\n",
      "MSE Loss 0.0000 CE_Loss: 1.3815 KL_Loss: -0.4078\n",
      "MSE Loss 0.0000 CE_Loss: 1.0189 KL_Loss: -0.3387\n",
      "MSE Loss 0.0000 CE_Loss: 0.8845 KL_Loss: -0.2942\n",
      "Training [53%]\tLoss: 0.0118\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 0.8912 KL_Loss: -0.3000\n",
      "MSE Loss 0.0000 CE_Loss: 1.3862 KL_Loss: -0.4091\n",
      "MSE Loss 0.0000 CE_Loss: 0.9481 KL_Loss: -0.3201\n",
      "MSE Loss 0.0000 CE_Loss: 1.3179 KL_Loss: -0.3876\n",
      "MSE Loss 0.0000 CE_Loss: 1.2534 KL_Loss: -0.3623\n",
      "Training [54%]\tLoss: 0.0116\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2212 KL_Loss: -0.3569\n",
      "MSE Loss 0.0000 CE_Loss: 1.2121 KL_Loss: -0.3491\n",
      "MSE Loss 0.0000 CE_Loss: 1.3852 KL_Loss: -0.4088\n",
      "MSE Loss 0.0000 CE_Loss: 0.9195 KL_Loss: -0.3035\n",
      "MSE Loss 0.0000 CE_Loss: 1.3367 KL_Loss: -0.3948\n",
      "Training [55%]\tLoss: 0.0122\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3823 KL_Loss: -0.4080\n",
      "MSE Loss 0.0000 CE_Loss: 1.3757 KL_Loss: -0.4065\n",
      "MSE Loss 0.0000 CE_Loss: 1.1304 KL_Loss: -0.3263\n",
      "MSE Loss 0.0000 CE_Loss: 1.1499 KL_Loss: -0.3577\n",
      "MSE Loss 0.0000 CE_Loss: 1.3168 KL_Loss: -0.3900\n",
      "Training [56%]\tLoss: 0.0127\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3851 KL_Loss: -0.4087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss 0.0000 CE_Loss: 1.2421 KL_Loss: -0.3749\n",
      "MSE Loss 0.0000 CE_Loss: 0.7998 KL_Loss: -0.2746\n",
      "MSE Loss 0.0000 CE_Loss: 0.8881 KL_Loss: -0.3003\n",
      "MSE Loss 0.0000 CE_Loss: 1.3279 KL_Loss: -0.3920\n",
      "Training [57%]\tLoss: 0.0113\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3204 KL_Loss: -0.3938\n",
      "MSE Loss 0.0000 CE_Loss: 1.0971 KL_Loss: -0.3505\n",
      "MSE Loss 0.0000 CE_Loss: 1.2548 KL_Loss: -0.3632\n",
      "MSE Loss 0.0000 CE_Loss: 1.2066 KL_Loss: -0.3650\n",
      "MSE Loss 0.0000 CE_Loss: 1.2863 KL_Loss: -0.3850\n",
      "Training [58%]\tLoss: 0.0123\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2674 KL_Loss: -0.3793\n",
      "MSE Loss 0.0000 CE_Loss: 1.0994 KL_Loss: -0.3348\n",
      "MSE Loss 0.0000 CE_Loss: 1.2788 KL_Loss: -0.3755\n",
      "MSE Loss 0.0000 CE_Loss: 1.3455 KL_Loss: -0.3995\n",
      "MSE Loss 0.0000 CE_Loss: 1.0321 KL_Loss: -0.3401\n",
      "Training [59%]\tLoss: 0.0121\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2476 KL_Loss: -0.3700\n",
      "MSE Loss 0.0000 CE_Loss: 1.1480 KL_Loss: -0.3587\n",
      "MSE Loss 0.0000 CE_Loss: 1.0078 KL_Loss: -0.3297\n",
      "MSE Loss 0.0000 CE_Loss: 1.1441 KL_Loss: -0.3094\n",
      "MSE Loss 0.0000 CE_Loss: 1.1737 KL_Loss: -0.3592\n",
      "Training [60%]\tLoss: 0.0114\n",
      "Learning rate: 0.00125\n",
      "MSE Loss 0.0000 CE_Loss: 1.0760 KL_Loss: -0.3361\n",
      "MSE Loss 0.0000 CE_Loss: 1.1117 KL_Loss: -0.3262\n",
      "MSE Loss 0.0000 CE_Loss: 1.2954 KL_Loss: -0.3871\n",
      "MSE Loss 0.0000 CE_Loss: 1.2653 KL_Loss: -0.3759\n",
      "MSE Loss 0.0000 CE_Loss: 1.2423 KL_Loss: -0.3745\n",
      "Training [61%]\tLoss: 0.0120\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 0.9661 KL_Loss: -0.3113\n",
      "MSE Loss 0.0000 CE_Loss: 1.1573 KL_Loss: -0.3395\n",
      "MSE Loss 0.0000 CE_Loss: 1.3357 KL_Loss: -0.3956\n",
      "MSE Loss 0.0000 CE_Loss: 1.2499 KL_Loss: -0.3805\n",
      "MSE Loss 0.0000 CE_Loss: 1.2620 KL_Loss: -0.3845\n",
      "Training [62%]\tLoss: 0.0119\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.0117 KL_Loss: -0.3240\n",
      "MSE Loss 0.0000 CE_Loss: 1.3810 KL_Loss: -0.4076\n",
      "MSE Loss 0.0000 CE_Loss: 1.2194 KL_Loss: -0.3694\n",
      "MSE Loss 0.0000 CE_Loss: 0.9807 KL_Loss: -0.3220\n",
      "MSE Loss 0.0000 CE_Loss: 1.0856 KL_Loss: -0.3323\n",
      "Training [63%]\tLoss: 0.0114\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.3492 KL_Loss: -0.3978\n",
      "MSE Loss 0.0000 CE_Loss: 1.2210 KL_Loss: -0.3457\n",
      "MSE Loss 0.0000 CE_Loss: 1.2997 KL_Loss: -0.3803\n",
      "MSE Loss 0.0000 CE_Loss: 1.3832 KL_Loss: -0.4082\n",
      "MSE Loss 0.0000 CE_Loss: 1.3744 KL_Loss: -0.4058\n",
      "Training [64%]\tLoss: 0.0133\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.0962 KL_Loss: -0.3426\n",
      "MSE Loss 0.0000 CE_Loss: 1.3632 KL_Loss: -0.4027\n",
      "MSE Loss 0.0000 CE_Loss: 0.9650 KL_Loss: -0.3192\n",
      "MSE Loss 0.0000 CE_Loss: 1.2445 KL_Loss: -0.3675\n",
      "MSE Loss 0.0000 CE_Loss: 1.2143 KL_Loss: -0.3468\n",
      "Training [65%]\tLoss: 0.0118\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.3823 KL_Loss: -0.4079\n",
      "MSE Loss 0.0000 CE_Loss: 1.1420 KL_Loss: -0.3537\n",
      "MSE Loss 0.0000 CE_Loss: 1.1212 KL_Loss: -0.3446\n",
      "MSE Loss 0.0000 CE_Loss: 1.3720 KL_Loss: -0.4049\n",
      "MSE Loss 0.0000 CE_Loss: 0.9057 KL_Loss: -0.2996\n",
      "Training [66%]\tLoss: 0.0118\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.3854 KL_Loss: -0.4088\n",
      "MSE Loss 0.0000 CE_Loss: 1.0977 KL_Loss: -0.2994\n",
      "MSE Loss 0.0000 CE_Loss: 1.3853 KL_Loss: -0.4088\n",
      "MSE Loss 0.0000 CE_Loss: 1.3112 KL_Loss: -0.3928\n",
      "MSE Loss 0.0000 CE_Loss: 1.1436 KL_Loss: -0.3458\n",
      "Training [67%]\tLoss: 0.0127\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.0267 KL_Loss: -0.3400\n",
      "MSE Loss 0.0000 CE_Loss: 0.8605 KL_Loss: -0.2976\n",
      "MSE Loss 0.0000 CE_Loss: 1.1707 KL_Loss: -0.3343\n",
      "MSE Loss 0.0000 CE_Loss: 1.0137 KL_Loss: -0.3110\n",
      "MSE Loss 0.0000 CE_Loss: 1.1660 KL_Loss: -0.3446\n",
      "Training [68%]\tLoss: 0.0105\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 1.3170 KL_Loss: -0.3937\n",
      "MSE Loss 0.0000 CE_Loss: 1.2202 KL_Loss: -0.3430\n",
      "MSE Loss 0.0000 CE_Loss: 1.3677 KL_Loss: -0.4036\n",
      "MSE Loss 0.0000 CE_Loss: 1.3779 KL_Loss: -0.4068\n",
      "MSE Loss 0.0000 CE_Loss: 1.3853 KL_Loss: -0.4088\n",
      "Training [69%]\tLoss: 0.0133\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 0.8492 KL_Loss: -0.2932\n",
      "MSE Loss 0.0000 CE_Loss: 1.1759 KL_Loss: -0.3139\n",
      "MSE Loss 0.0000 CE_Loss: 1.3383 KL_Loss: -0.3942\n",
      "MSE Loss 0.0000 CE_Loss: 0.8573 KL_Loss: -0.2919\n",
      "MSE Loss 0.0000 CE_Loss: 0.8001 KL_Loss: -0.2771\n",
      "Training [70%]\tLoss: 0.0100\n",
      "Learning rate: 0.000625\n",
      "MSE Loss 0.0000 CE_Loss: 0.9990 KL_Loss: -0.3291\n",
      "MSE Loss 0.0000 CE_Loss: 0.7969 KL_Loss: -0.2718\n",
      "MSE Loss 0.0000 CE_Loss: 0.9340 KL_Loss: -0.3187\n",
      "MSE Loss 0.0000 CE_Loss: 1.3862 KL_Loss: -0.4091\n",
      "MSE Loss 0.0000 CE_Loss: 0.9876 KL_Loss: -0.3268\n",
      "Training [71%]\tLoss: 0.0102\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3540 KL_Loss: -0.4001\n",
      "MSE Loss 0.0000 CE_Loss: 1.2062 KL_Loss: -0.3530\n",
      "MSE Loss 0.0000 CE_Loss: 1.2018 KL_Loss: -0.3466\n",
      "MSE Loss 0.0000 CE_Loss: 1.3583 KL_Loss: -0.4013\n",
      "MSE Loss 0.0000 CE_Loss: 1.2254 KL_Loss: -0.3575\n",
      "Training [72%]\tLoss: 0.0127\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 0.8693 KL_Loss: -0.2996\n",
      "MSE Loss 0.0000 CE_Loss: 1.1273 KL_Loss: -0.3551\n",
      "MSE Loss 0.0000 CE_Loss: 1.1354 KL_Loss: -0.3547\n",
      "MSE Loss 0.0000 CE_Loss: 1.2284 KL_Loss: -0.3729\n",
      "MSE Loss 0.0000 CE_Loss: 1.3862 KL_Loss: -0.4090\n",
      "Training [73%]\tLoss: 0.0115\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2548 KL_Loss: -0.3763\n",
      "MSE Loss 0.0000 CE_Loss: 1.3843 KL_Loss: -0.4085\n",
      "MSE Loss 0.0000 CE_Loss: 1.3114 KL_Loss: -0.3905\n",
      "MSE Loss 0.0000 CE_Loss: 1.2009 KL_Loss: -0.3378\n",
      "MSE Loss 0.0000 CE_Loss: 1.3471 KL_Loss: -0.3968\n",
      "Training [74%]\tLoss: 0.0130\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2116 KL_Loss: -0.3711\n",
      "MSE Loss 0.0000 CE_Loss: 1.2855 KL_Loss: -0.3851\n",
      "MSE Loss 0.0001 CE_Loss: 1.2681 KL_Loss: -0.3743\n",
      "MSE Loss 0.0000 CE_Loss: 1.1385 KL_Loss: -0.3439\n",
      "MSE Loss 0.0000 CE_Loss: 1.2075 KL_Loss: -0.3625\n",
      "Training [75%]\tLoss: 0.0122\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2809 KL_Loss: -0.3738\n",
      "MSE Loss 0.0000 CE_Loss: 1.2848 KL_Loss: -0.3744\n",
      "MSE Loss 0.0000 CE_Loss: 1.3731 KL_Loss: -0.4052\n",
      "MSE Loss 0.0000 CE_Loss: 1.3516 KL_Loss: -0.3986\n",
      "MSE Loss 0.0000 CE_Loss: 1.3583 KL_Loss: -0.4019\n",
      "Training [76%]\tLoss: 0.0133\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.0174 KL_Loss: -0.3302\n",
      "MSE Loss 0.0000 CE_Loss: 1.3738 KL_Loss: -0.4062\n",
      "MSE Loss 0.0000 CE_Loss: 1.3265 KL_Loss: -0.3933\n",
      "MSE Loss 0.0000 CE_Loss: 1.2754 KL_Loss: -0.3779\n",
      "MSE Loss 0.0000 CE_Loss: 1.1236 KL_Loss: -0.3409\n",
      "Training [77%]\tLoss: 0.0122\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3373 KL_Loss: -0.3964\n",
      "MSE Loss 0.0000 CE_Loss: 1.3502 KL_Loss: -0.3989\n",
      "MSE Loss 0.0000 CE_Loss: 1.2487 KL_Loss: -0.3706\n",
      "MSE Loss 0.0000 CE_Loss: 1.3817 KL_Loss: -0.4077\n",
      "MSE Loss 0.0000 CE_Loss: 1.3399 KL_Loss: -0.3963\n",
      "Training [78%]\tLoss: 0.0133\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3186 KL_Loss: -0.3928\n",
      "MSE Loss 0.0000 CE_Loss: 1.3683 KL_Loss: -0.4040\n",
      "MSE Loss 0.0000 CE_Loss: 1.0703 KL_Loss: -0.3271\n",
      "MSE Loss 0.0000 CE_Loss: 1.2477 KL_Loss: -0.3685\n",
      "MSE Loss 0.0000 CE_Loss: 1.3671 KL_Loss: -0.4035\n",
      "Training [79%]\tLoss: 0.0128\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.1816 KL_Loss: -0.3444\n",
      "MSE Loss 0.0000 CE_Loss: 1.0964 KL_Loss: -0.3532\n",
      "MSE Loss 0.0000 CE_Loss: 1.3713 KL_Loss: -0.4050\n",
      "MSE Loss 0.0000 CE_Loss: 1.3570 KL_Loss: -0.4010\n",
      "MSE Loss 0.0000 CE_Loss: 1.1438 KL_Loss: -0.3325\n",
      "Training [80%]\tLoss: 0.0123\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2790 KL_Loss: -0.3860\n",
      "MSE Loss 0.0000 CE_Loss: 1.1857 KL_Loss: -0.3613\n",
      "MSE Loss 0.0000 CE_Loss: 1.2193 KL_Loss: -0.3766\n",
      "MSE Loss 0.0000 CE_Loss: 0.8322 KL_Loss: -0.2794\n",
      "MSE Loss 0.0000 CE_Loss: 1.2224 KL_Loss: -0.3677\n",
      "Training [81%]\tLoss: 0.0115\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 0.9903 KL_Loss: -0.3230\n",
      "MSE Loss 0.0000 CE_Loss: 1.2005 KL_Loss: -0.3464\n",
      "MSE Loss 0.0000 CE_Loss: 0.9080 KL_Loss: -0.3037\n",
      "MSE Loss 0.0000 CE_Loss: 1.2749 KL_Loss: -0.3779\n",
      "MSE Loss 0.0000 CE_Loss: 1.0901 KL_Loss: -0.3477\n",
      "Training [82%]\tLoss: 0.0109\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.1152 KL_Loss: -0.3247\n",
      "MSE Loss 0.0000 CE_Loss: 1.3014 KL_Loss: -0.3861\n",
      "MSE Loss 0.0000 CE_Loss: 1.3766 KL_Loss: -0.4064\n",
      "MSE Loss 0.0000 CE_Loss: 1.3197 KL_Loss: -0.3881\n",
      "MSE Loss 0.0000 CE_Loss: 1.2760 KL_Loss: -0.3808\n",
      "Training [83%]\tLoss: 0.0128\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3861 KL_Loss: -0.4090\n",
      "MSE Loss 0.0000 CE_Loss: 1.1998 KL_Loss: -0.3644\n",
      "MSE Loss 0.0000 CE_Loss: 1.3374 KL_Loss: -0.3944\n",
      "MSE Loss 0.0000 CE_Loss: 1.3841 KL_Loss: -0.4085\n",
      "MSE Loss 0.0000 CE_Loss: 1.1691 KL_Loss: -0.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [84%]\tLoss: 0.0130\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2213 KL_Loss: -0.3515\n",
      "MSE Loss 0.0000 CE_Loss: 1.3846 KL_Loss: -0.4086\n",
      "MSE Loss 0.0000 CE_Loss: 1.3098 KL_Loss: -0.3896\n",
      "MSE Loss 0.0000 CE_Loss: 1.2630 KL_Loss: -0.3695\n",
      "MSE Loss 0.0000 CE_Loss: 1.3722 KL_Loss: -0.4049\n",
      "Training [85%]\tLoss: 0.0131\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 0.8350 KL_Loss: -0.2867\n",
      "MSE Loss 0.0000 CE_Loss: 1.1941 KL_Loss: -0.3586\n",
      "MSE Loss 0.0000 CE_Loss: 1.3166 KL_Loss: -0.3922\n",
      "MSE Loss 0.0000 CE_Loss: 1.3805 KL_Loss: -0.4075\n",
      "MSE Loss 0.0000 CE_Loss: 1.3656 KL_Loss: -0.4035\n",
      "Training [86%]\tLoss: 0.0122\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2260 KL_Loss: -0.3645\n",
      "MSE Loss 0.0000 CE_Loss: 1.1998 KL_Loss: -0.3340\n",
      "MSE Loss 0.0000 CE_Loss: 0.9488 KL_Loss: -0.2993\n",
      "MSE Loss 0.0000 CE_Loss: 0.9008 KL_Loss: -0.3094\n",
      "MSE Loss 0.0000 CE_Loss: 1.2663 KL_Loss: -0.3757\n",
      "Training [87%]\tLoss: 0.0111\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3388 KL_Loss: -0.3948\n",
      "MSE Loss 0.0000 CE_Loss: 1.1887 KL_Loss: -0.3528\n",
      "MSE Loss 0.0000 CE_Loss: 1.1454 KL_Loss: -0.3550\n",
      "MSE Loss 0.0000 CE_Loss: 1.2095 KL_Loss: -0.3391\n",
      "MSE Loss 0.0000 CE_Loss: 1.3862 KL_Loss: -0.4090\n",
      "Training [88%]\tLoss: 0.0125\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2660 KL_Loss: -0.3679\n",
      "MSE Loss 0.0000 CE_Loss: 1.3194 KL_Loss: -0.3923\n",
      "MSE Loss 0.0000 CE_Loss: 1.3844 KL_Loss: -0.4085\n",
      "MSE Loss 0.0000 CE_Loss: 1.3773 KL_Loss: -0.4065\n",
      "MSE Loss 0.0000 CE_Loss: 1.3426 KL_Loss: -0.3962\n",
      "Training [89%]\tLoss: 0.0134\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2302 KL_Loss: -0.3633\n",
      "MSE Loss 0.0000 CE_Loss: 1.3556 KL_Loss: -0.4001\n",
      "MSE Loss 0.0000 CE_Loss: 1.3662 KL_Loss: -0.4037\n",
      "MSE Loss 0.0000 CE_Loss: 1.3759 KL_Loss: -0.4062\n",
      "MSE Loss 0.0000 CE_Loss: 1.3421 KL_Loss: -0.3970\n",
      "Training [90%]\tLoss: 0.0133\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2364 KL_Loss: -0.3690\n",
      "MSE Loss 0.0000 CE_Loss: 1.3862 KL_Loss: -0.4091\n",
      "MSE Loss 0.0000 CE_Loss: 1.3122 KL_Loss: -0.3870\n",
      "MSE Loss 0.0000 CE_Loss: 1.0240 KL_Loss: -0.3152\n",
      "MSE Loss 0.0000 CE_Loss: 1.2480 KL_Loss: -0.3682\n",
      "Training [91%]\tLoss: 0.0124\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3197 KL_Loss: -0.3894\n",
      "MSE Loss 0.0000 CE_Loss: 1.2241 KL_Loss: -0.3452\n",
      "MSE Loss 0.0000 CE_Loss: 0.9064 KL_Loss: -0.3027\n",
      "MSE Loss 0.0000 CE_Loss: 1.3776 KL_Loss: -0.4066\n",
      "MSE Loss 0.0000 CE_Loss: 1.2399 KL_Loss: -0.3730\n",
      "Training [92%]\tLoss: 0.0121\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2531 KL_Loss: -0.3739\n",
      "MSE Loss 0.0000 CE_Loss: 1.3245 KL_Loss: -0.3899\n",
      "MSE Loss 0.0000 CE_Loss: 1.3840 KL_Loss: -0.4085\n",
      "MSE Loss 0.0000 CE_Loss: 1.0680 KL_Loss: -0.3476\n",
      "MSE Loss 0.0000 CE_Loss: 0.9737 KL_Loss: -0.3230\n",
      "Training [93%]\tLoss: 0.0120\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.1997 KL_Loss: -0.3301\n",
      "MSE Loss 0.0000 CE_Loss: 1.0761 KL_Loss: -0.3139\n",
      "MSE Loss 0.0000 CE_Loss: 1.3190 KL_Loss: -0.3887\n",
      "MSE Loss 0.0000 CE_Loss: 1.3731 KL_Loss: -0.4053\n",
      "MSE Loss 0.0000 CE_Loss: 1.2917 KL_Loss: -0.3791\n",
      "Training [94%]\tLoss: 0.0125\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3065 KL_Loss: -0.3882\n",
      "MSE Loss 0.0000 CE_Loss: 1.2432 KL_Loss: -0.3728\n",
      "MSE Loss 0.0000 CE_Loss: 1.0648 KL_Loss: -0.3114\n",
      "MSE Loss 0.0000 CE_Loss: 1.3227 KL_Loss: -0.3890\n",
      "MSE Loss 0.0000 CE_Loss: 1.1713 KL_Loss: -0.3288\n",
      "Training [95%]\tLoss: 0.0122\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.3066 KL_Loss: -0.3906\n",
      "MSE Loss 0.0000 CE_Loss: 1.3600 KL_Loss: -0.4012\n",
      "MSE Loss 0.0000 CE_Loss: 1.3524 KL_Loss: -0.3993\n",
      "MSE Loss 0.0000 CE_Loss: 1.2822 KL_Loss: -0.3822\n",
      "MSE Loss 0.0000 CE_Loss: 1.3206 KL_Loss: -0.3913\n",
      "Training [96%]\tLoss: 0.0132\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2594 KL_Loss: -0.3655\n",
      "MSE Loss 0.0000 CE_Loss: 1.3380 KL_Loss: -0.3947\n",
      "MSE Loss 0.0000 CE_Loss: 1.2999 KL_Loss: -0.3882\n",
      "MSE Loss 0.0000 CE_Loss: 1.3310 KL_Loss: -0.3931\n",
      "MSE Loss 0.0000 CE_Loss: 0.9987 KL_Loss: -0.3166\n",
      "Training [97%]\tLoss: 0.0125\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.2396 KL_Loss: -0.3728\n",
      "MSE Loss 0.0000 CE_Loss: 1.3249 KL_Loss: -0.3901\n",
      "MSE Loss 0.0000 CE_Loss: 1.2787 KL_Loss: -0.3811\n",
      "MSE Loss 0.0000 CE_Loss: 1.3030 KL_Loss: -0.3826\n",
      "MSE Loss 0.0000 CE_Loss: 1.1216 KL_Loss: -0.3186\n",
      "Training [98%]\tLoss: 0.0125\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 1.0935 KL_Loss: -0.3315\n",
      "MSE Loss 0.0000 CE_Loss: 1.1699 KL_Loss: -0.3386\n",
      "MSE Loss 0.0000 CE_Loss: 1.0403 KL_Loss: -0.3401\n",
      "MSE Loss 0.0000 CE_Loss: 1.3088 KL_Loss: -0.3910\n",
      "MSE Loss 0.0000 CE_Loss: 1.3786 KL_Loss: -0.4068\n",
      "Training [99%]\tLoss: 0.0120\n",
      "Learning rate: 0.0003125\n",
      "MSE Loss 0.0000 CE_Loss: 0.9326 KL_Loss: -0.3141\n",
      "MSE Loss 0.0000 CE_Loss: 1.3366 KL_Loss: -0.3960\n",
      "MSE Loss 0.0000 CE_Loss: 0.8690 KL_Loss: -0.3012\n",
      "MSE Loss 0.0000 CE_Loss: 1.2673 KL_Loss: -0.3737\n",
      "MSE Loss 0.0000 CE_Loss: 1.0179 KL_Loss: -0.3301\n",
      "Training [100%]\tLoss: 0.0109\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# Define optimizer and loss\n",
    "lr = 0.08\n",
    "lr_min = 0.0005\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "mse_loss = MSELoss(reduction=\"sum\")\n",
    "ce_loss = CrossEntropyLoss() \n",
    "kl_div_loss = KLDivLoss()\n",
    "\n",
    "# Start training\n",
    "student_model.train()  # set model to training mode\n",
    "\n",
    "\n",
    "# Note from (https://pytorch.org/docs/stable/optim.html):\n",
    "# Some optimization algorithms such as LBFGS need to\n",
    "# reevaluate the function multiple times, so you have to\n",
    "# pass in a closure that allows them to recompute your model.\n",
    "# The closure should clear the gradients, compute the loss,\n",
    "# and return it.\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "loss_list = [] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    \n",
    "    print(\"Learning rate:\", lr )\n",
    "    \n",
    "    for batch_idx in range(5):\n",
    "        \n",
    "        input_data = np.random.uniform(low=0.0, high=np.pi, size=(1,2))\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "        \n",
    "        output = student_model(Tensor(input_data))  # Forward pass\n",
    "        target = Tensor(qnn_teacher.forward(input_data, [])) # took a Qiskit QNN object defined above\n",
    "        #teacher_model(Tensor(input_data))\n",
    "        #print(target)\n",
    "        mse_l = mse_loss(output, target)\n",
    "        ce_l = ce_loss(output, target) \n",
    "        kl_div_l = kl_div_loss(output, target) \n",
    "        \n",
    "        print(\"MSE Loss {:.4f} CE_Loss: {:.4f} KL_Loss: {:.4f}\".format(mse_l, ce_l, kl_div_l))\n",
    "        \n",
    "        loss = 1.* mse_l + 0.01*ce_l + 0.*kl_div_l # Calculate loss\n",
    "        \n",
    "        #print(loss)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize weights\n",
    "        total_loss.append(loss.item())  # Store loss\n",
    "    loss_list.append(sum(total_loss) / len(total_loss))\n",
    "    \n",
    "    if epoch % 10 == 0 and lr > lr_min :\n",
    "        lr = lr / 2.\n",
    "    \n",
    "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4271f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.5702e+00, -1.8719e-03, -4.1066e-03,  2.7337e-02,  1.5715e+00,\n",
       "        -6.8814e-04, -5.0672e-03, -2.6600e-02], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38c59bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title(\"Distillation QNN trainin\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152edd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
